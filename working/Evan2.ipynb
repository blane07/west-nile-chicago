{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original code: Beating the Benchmark from West Nile Virus Prediction @ Kaggle by Abhihsek. Modified by Brendan Lane*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('.nbp-app-bar').toggle()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('.nbp-app-bar').toggle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "train = pd.read_csv('../assets/train.csv')\n",
    "test = pd.read_csv('../assets/test.csv')\n",
    "sample = pd.read_csv('../assets/sampleSubmission.csv')\n",
    "weather = pd.read_csv('../assets/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not using codesum for this benchmark\n",
    "weather.drop('CodeSum', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split station 1 and 2 and join horizontally\n",
    "weather_stn1 = weather[weather['Station']==1]\n",
    "weather_stn2 = weather[weather['Station']==2]\n",
    "weather_stn1 = weather_stn1.drop('Station', axis=1)\n",
    "weather_stn2 = weather_stn2.drop('Station', axis=1)\n",
    "weather = weather_stn1.merge(weather_stn2, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace some missing values and T with -1\n",
    "weather = weather.replace('M', -1)\n",
    "weather = weather.replace('-', -1)\n",
    "weather = weather.replace('T', -1)\n",
    "weather = weather.replace(' T', -1)\n",
    "weather = weather.replace('  T', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to extract month and day from dataset\n",
    "# You can also use parse_dates of Pandas.\n",
    "def create_year(x):\n",
    "    return x.split('-')[0]\n",
    "def create_month(x):\n",
    "    return x.split('-')[1]\n",
    "def create_day(x):\n",
    "    return x.split('-')[2]\n",
    "\n",
    "train['month'] = train.Date.apply(create_month)\n",
    "train['day'] = train.Date.apply(create_day)\n",
    "train['year'] = train.Date.apply(create_year)\n",
    "\n",
    "test['month'] = test.Date.apply(create_month)\n",
    "test['day'] = test.Date.apply(create_day)\n",
    "test['year'] = test.Date.apply(create_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop address columns\n",
    "train = train.drop(['Address', 'AddressNumberAndStreet'], axis = 1)\n",
    "test = test.drop(['Id', 'Address', 'AddressNumberAndStreet'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with weather data\n",
    "train = train.merge(weather, on='Date')\n",
    "test = test.merge(weather, on='Date')\n",
    "train = train.drop(['Date'], axis = 1)\n",
    "test = test.drop(['Date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numbers\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(list(train['Species'].values) + list(test['Species'].values))\n",
    "train['Species'] = lbl.transform(train['Species'].values)\n",
    "test['Species'] = lbl.transform(test['Species'].values)\n",
    "\n",
    "lbl.fit(list(train['Street'].values) + list(test['Street'].values))\n",
    "train['Street'] = lbl.transform(train['Street'].values)\n",
    "test['Street'] = lbl.transform(test['Street'].values)\n",
    "\n",
    "lbl.fit(list(train['Trap'].values) + list(test['Trap'].values))\n",
    "train['Trap'] = lbl.transform(train['Trap'].values)\n",
    "test['Trap'] = lbl.transform(test['Trap'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with -1s\n",
    "train = train.loc[:,(train != -1).any(axis=0)]\n",
    "test = test.loc[:,(test != -1).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert all data types to floats\n",
    "train = train.astype(float)\n",
    "test = test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature engineering\n",
    "# Temperature difference\n",
    "train['temp_delta_x'] = train.Tmax_x - train.Tmin_x\n",
    "train['temp_delta_y'] = train.Tmax_y - train.Tmin_y\n",
    "train['temp_delta_x'] = train.Tmax_x - train.Tmin_x\n",
    "train['temp_delta_y'] = train.Tmax_y - train.Tmin_y\n",
    "test['temp_delta_x'] = test.Tmax_x - test.Tmin_x\n",
    "test['temp_delta_y'] = test.Tmax_y - test.Tmin_y\n",
    "test['temp_delta_x'] = test.Tmax_x - test.Tmin_x\n",
    "test['temp_delta_y'] = test.Tmax_y - test.Tmin_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is raining?\n",
    "train['israining_x'] = (train.PrecipTotal_x > 0).astype(int)\n",
    "train['israining_y'] = (train.PrecipTotal_y > 0).astype(int)\n",
    "test['israining_x'] = (test.PrecipTotal_x > 0).astype(int)\n",
    "test['israining_y'] = (test.PrecipTotal_y > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of sunlight\n",
    "train['sunlight'] = train.Sunset_x - train.Sunrise_x\n",
    "test['sunlight'] = test.Sunset_x - test.Sunrise_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers with median value\n",
    "train.WetBulb_x = train.WetBulb_x.apply(lambda x: train.WetBulb_x.median() if x < 0 else x)\n",
    "test.WetBulb_x = test.WetBulb_x.apply(lambda x: test.WetBulb_x.median() if x < 0 else x)\n",
    "\n",
    "train.StnPressure_x = train.StnPressure_x.apply(lambda x: train.StnPressure_x.median() if x < 0 else x)\n",
    "test.StnPressure_x = test.StnPressure_x.apply(lambda x: test.StnPressure_x.median() if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative humidity approximation\n",
    "train['RH'] = 100 - (25 / 9) * (train.Tavg_x - train.DewPoint_x)\n",
    "test['RH'] = 100 - (25 / 9) * (test.Tavg_x - test.DewPoint_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is heat wave?\n",
    "train['isheat'] = (train.Heat_x > 0).astype(float)\n",
    "test['isheat'] = (test.Heat_x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize resultant wind into north and east components\n",
    "wind_dir_north_x = train.ResultDir_x.apply(lambda x: np.sin(np.deg2rad(x)))\n",
    "wind_dir_east_x = train.ResultDir_x.apply(lambda x: np.cos(np.deg2rad(x)))\n",
    "train['wind_north_x'] = wind_dir_north_x * train.ResultSpeed_x\n",
    "train['wind_east_x'] = wind_dir_east_x * train.ResultSpeed_x\n",
    "\n",
    "wind_dir_north_y = train.ResultDir_y.apply(lambda x: np.sin(np.deg2rad(x)))\n",
    "wind_dir_east_y = train.ResultDir_y.apply(lambda x: np.cos(np.deg2rad(x)))\n",
    "train['wind_north_y'] = wind_dir_north_y * train.ResultSpeed_y\n",
    "train['wind_east_y'] = wind_dir_east_y * train.ResultSpeed_y\n",
    "\n",
    "wind_dir_north_x = test.ResultDir_x.apply(lambda x: np.sin(np.deg2rad(x)))\n",
    "wind_dir_east_x = test.ResultDir_x.apply(lambda x: np.cos(np.deg2rad(x)))\n",
    "test['wind_north_x'] = wind_dir_north_x * test.ResultSpeed_x\n",
    "test['wind_east_x'] = wind_dir_east_x * test.ResultSpeed_x\n",
    "\n",
    "wind_dir_north_y = test.ResultDir_y.apply(lambda x: np.sin(np.deg2rad(x)))\n",
    "wind_dir_east_y = test.ResultDir_y.apply(lambda x: np.cos(np.deg2rad(x)))\n",
    "test['wind_north_y'] = wind_dir_north_y * test.ResultSpeed_y\n",
    "test['wind_east_y'] = wind_dir_east_y * test.ResultSpeed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize average wind into north and east components\n",
    "train['avg_wind_north_x'] = wind_dir_north_x * train.AvgSpeed_x\n",
    "train['avg_wind_east_x'] = wind_dir_east_x * train.AvgSpeed_x\n",
    "\n",
    "train['avg_wind_north_y'] = wind_dir_north_y * train.AvgSpeed_y\n",
    "train['avg_wind_east_y'] = wind_dir_east_y * train.AvgSpeed_y\n",
    "\n",
    "test['avg_wind_north_x'] = wind_dir_north_x * test.AvgSpeed_x\n",
    "test['avg_wind_east_x'] = wind_dir_east_x * test.AvgSpeed_x\n",
    "\n",
    "test['avg_wind_north_y'] = wind_dir_north_y * test.AvgSpeed_y\n",
    "test['avg_wind_east_y'] = wind_dir_east_y * test.AvgSpeed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train.reset_index(), (pd.get_dummies(train.year)).reset_index(), (pd.get_dummies(train.month)).reset_index(), (pd.get_dummies(train.Species)).reset_index()], axis='columns')\n",
    "test = pd.concat([test.reset_index(), (pd.get_dummies(test.year)).reset_index(), (pd.get_dummies(test.month)).reset_index(), (pd.get_dummies(test.Species)).reset_index()], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['PrecipTotal_x', 'PrecipTotal_y', 'Sunrise_x', 'Sunset_x',\n",
    "             'Heat_x', 'Depth_x', 'SnowFall_x', 'ResultDir_x', 'ResultSpeed_x',\n",
    "             'ResultDir_y', 'ResultSpeed_y', 'AvgSpeed_x', 'AvgSpeed_y', 'year',\n",
    "             'month', 'Species']\n",
    "\n",
    "train.drop(drop_list, axis='columns', inplace=True)\n",
    "test.drop(drop_list, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dependent (y) and indepedent (X) variables\n",
    "y = train.WnvPresent\n",
    "X = train.drop('WnvPresent', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475537787930707"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline accuracy\n",
    "1 - y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X for modeling\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475250626167533"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest\n",
    "cross_val_score(RandomForestClassifier(max_depth=4, max_features=5), X_train, y_train,\n",
    "                cv=5, n_jobs=-1, verbose=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Doesn't break baseline. What is happening?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1992,    0],\n",
       "       [ 110,    0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confustion matrix\n",
    "clf = RandomForestClassifier(max_depth=4, max_features=5)\n",
    "clf.fit(X_train, y_train)\n",
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The model is maximizing accuracy by predicting \"No\" everytime. This is the result of have unbalanced classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of positive West Nile results\n",
    "pos_train = train[train.WnvPresent == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled unrepresented class\n",
    "traino = pd.concat([train, pos_train], axis=0)\n",
    "for _ in range(10):\n",
    "    traino = pd.concat([traino, pos_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled train/test split\n",
    "yo = traino.WnvPresent\n",
    "Xo = traino.drop('WnvPresent', axis='columns')\n",
    "Xo = pd.DataFrame(scaler.fit_transform(Xo), columns=Xo.columns)\n",
    "X_traino, X_testo, y_traino, y_testo = train_test_split(Xo, yo, test_size=0.2, stratify=yo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to maximize recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1561  431]\n",
      " [  37   73]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.78      0.87      1992\n",
      "        Yes       0.14      0.66      0.24       110\n",
      "\n",
      "avg / total       0.93      0.78      0.84      2102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=4, max_features=7)\n",
    "clf.fit(X_traino, y_traino)\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "print(classification_report(y_test, clf.predict(X_test), target_names=['No', 'Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "input_units = X_train.shape[1]\n",
    "hidden_units = input_units\n",
    "\n",
    "model.add(Dense(hidden_units * 2,\n",
    "                input_dim=input_units,\n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(hidden_units * 2,\n",
    "                input_dim=input_units,\n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10,\n",
    "                input_dim=input_units,\n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13253 samples, validate on 2102 samples\n",
      "Epoch 1/750\n",
      "13253/13253 [==============================] - 1s 94us/step - loss: 0.5188 - acc: 0.7303 - val_loss: 0.4368 - val_acc: 0.7512\n",
      "Epoch 2/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.4367 - acc: 0.7896 - val_loss: 0.4927 - val_acc: 0.7331\n",
      "Epoch 3/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.4150 - acc: 0.7991 - val_loss: 0.4928 - val_acc: 0.6912\n",
      "Epoch 4/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.3972 - acc: 0.8080 - val_loss: 0.4665 - val_acc: 0.7022\n",
      "Epoch 5/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.3824 - acc: 0.8123 - val_loss: 0.4853 - val_acc: 0.6855\n",
      "Epoch 6/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.3782 - acc: 0.8186 - val_loss: 0.5139 - val_acc: 0.6646\n",
      "Epoch 7/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.3711 - acc: 0.8198 - val_loss: 0.4809 - val_acc: 0.6903\n",
      "Epoch 8/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.3644 - acc: 0.8215 - val_loss: 0.4213 - val_acc: 0.7331\n",
      "Epoch 9/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.3545 - acc: 0.8271 - val_loss: 0.4173 - val_acc: 0.7417\n",
      "Epoch 10/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.3466 - acc: 0.8351 - val_loss: 0.4608 - val_acc: 0.6998\n",
      "Epoch 11/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.3362 - acc: 0.8395 - val_loss: 0.4424 - val_acc: 0.7165\n",
      "Epoch 12/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.3280 - acc: 0.8470 - val_loss: 0.4469 - val_acc: 0.7236\n",
      "Epoch 13/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.3227 - acc: 0.8468 - val_loss: 0.4065 - val_acc: 0.7445\n",
      "Epoch 14/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.3225 - acc: 0.8499 - val_loss: 0.4129 - val_acc: 0.7379\n",
      "Epoch 15/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.3132 - acc: 0.8563 - val_loss: 0.4266 - val_acc: 0.7303\n",
      "Epoch 16/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.3104 - acc: 0.8563 - val_loss: 0.4181 - val_acc: 0.7217\n",
      "Epoch 17/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.2952 - acc: 0.8643 - val_loss: 0.4090 - val_acc: 0.7307\n",
      "Epoch 18/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.2927 - acc: 0.8658 - val_loss: 0.3653 - val_acc: 0.7588\n",
      "Epoch 19/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.2870 - acc: 0.8695 - val_loss: 0.3663 - val_acc: 0.7598\n",
      "Epoch 20/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.2848 - acc: 0.8732 - val_loss: 0.3438 - val_acc: 0.7712\n",
      "Epoch 21/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.2789 - acc: 0.8781 - val_loss: 0.3889 - val_acc: 0.7464\n",
      "Epoch 22/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.2723 - acc: 0.8787 - val_loss: 0.3747 - val_acc: 0.7636\n",
      "Epoch 23/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.2744 - acc: 0.8789 - val_loss: 0.3755 - val_acc: 0.7716\n",
      "Epoch 24/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.2726 - acc: 0.8789 - val_loss: 0.3621 - val_acc: 0.7764\n",
      "Epoch 25/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.2678 - acc: 0.8807 - val_loss: 0.3617 - val_acc: 0.7774\n",
      "Epoch 26/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.2578 - acc: 0.8883 - val_loss: 0.3797 - val_acc: 0.7807\n",
      "Epoch 27/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2586 - acc: 0.8893 - val_loss: 0.3387 - val_acc: 0.7850\n",
      "Epoch 28/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.2552 - acc: 0.8907 - val_loss: 0.3900 - val_acc: 0.7702\n",
      "Epoch 29/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.2525 - acc: 0.8906 - val_loss: 0.3992 - val_acc: 0.7712\n",
      "Epoch 30/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.2403 - acc: 0.8980 - val_loss: 0.3372 - val_acc: 0.8064\n",
      "Epoch 31/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.2489 - acc: 0.8931 - val_loss: 0.3459 - val_acc: 0.7973\n",
      "Epoch 32/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2416 - acc: 0.8955 - val_loss: 0.3677 - val_acc: 0.7969\n",
      "Epoch 33/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.2379 - acc: 0.8979 - val_loss: 0.3629 - val_acc: 0.7964\n",
      "Epoch 34/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2337 - acc: 0.9003 - val_loss: 0.3166 - val_acc: 0.8130\n",
      "Epoch 35/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.2298 - acc: 0.9008 - val_loss: 0.3009 - val_acc: 0.8164\n",
      "Epoch 36/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.2345 - acc: 0.9025 - val_loss: 0.3306 - val_acc: 0.8140\n",
      "Epoch 37/750\n",
      "13253/13253 [==============================] - 1s 63us/step - loss: 0.2251 - acc: 0.9082 - val_loss: 0.3048 - val_acc: 0.8306\n",
      "Epoch 38/750\n",
      "13253/13253 [==============================] - 1s 54us/step - loss: 0.2282 - acc: 0.9042 - val_loss: 0.3417 - val_acc: 0.8116\n",
      "Epoch 39/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.2278 - acc: 0.9040 - val_loss: 0.3000 - val_acc: 0.8306\n",
      "Epoch 40/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.2276 - acc: 0.9070 - val_loss: 0.3356 - val_acc: 0.8264\n",
      "Epoch 41/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.2194 - acc: 0.9094 - val_loss: 0.3106 - val_acc: 0.8349\n",
      "Epoch 42/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.2113 - acc: 0.9149 - val_loss: 0.3338 - val_acc: 0.8149\n",
      "Epoch 43/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2123 - acc: 0.9134 - val_loss: 0.2763 - val_acc: 0.8406\n",
      "Epoch 44/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2190 - acc: 0.9128 - val_loss: 0.3268 - val_acc: 0.8197\n",
      "Epoch 45/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2156 - acc: 0.9073 - val_loss: 0.2876 - val_acc: 0.8283\n",
      "Epoch 46/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.2048 - acc: 0.9174 - val_loss: 0.2882 - val_acc: 0.8406\n",
      "Epoch 47/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.2070 - acc: 0.9158 - val_loss: 0.2962 - val_acc: 0.8416\n",
      "Epoch 48/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.2033 - acc: 0.9185 - val_loss: 0.2594 - val_acc: 0.8578\n",
      "Epoch 49/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.2090 - acc: 0.9146 - val_loss: 0.2892 - val_acc: 0.8435\n",
      "Epoch 50/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.2061 - acc: 0.9147 - val_loss: 0.3009 - val_acc: 0.8497\n",
      "Epoch 51/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.1998 - acc: 0.9200 - val_loss: 0.3063 - val_acc: 0.8421\n",
      "Epoch 52/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.2033 - acc: 0.9180 - val_loss: 0.2786 - val_acc: 0.8530\n",
      "Epoch 53/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1954 - acc: 0.9230 - val_loss: 0.2923 - val_acc: 0.8425\n",
      "Epoch 54/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.2030 - acc: 0.9192 - val_loss: 0.2784 - val_acc: 0.8425\n",
      "Epoch 55/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1979 - acc: 0.9199 - val_loss: 0.2732 - val_acc: 0.8492\n",
      "Epoch 56/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.1963 - acc: 0.9208 - val_loss: 0.2465 - val_acc: 0.8673\n",
      "Epoch 57/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.2062 - acc: 0.9208 - val_loss: 0.3070 - val_acc: 0.8506\n",
      "Epoch 58/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1988 - acc: 0.9201 - val_loss: 0.2718 - val_acc: 0.8549\n",
      "Epoch 59/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 1s 60us/step - loss: 0.1916 - acc: 0.9241 - val_loss: 0.2700 - val_acc: 0.8587\n",
      "Epoch 60/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1904 - acc: 0.9243 - val_loss: 0.2649 - val_acc: 0.8563\n",
      "Epoch 61/750\n",
      "13253/13253 [==============================] - 1s 60us/step - loss: 0.1882 - acc: 0.9258 - val_loss: 0.2717 - val_acc: 0.8616\n",
      "Epoch 62/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.1900 - acc: 0.9241 - val_loss: 0.2900 - val_acc: 0.8516\n",
      "Epoch 63/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1827 - acc: 0.9277 - val_loss: 0.2780 - val_acc: 0.8620\n",
      "Epoch 64/750\n",
      "13253/13253 [==============================] - 1s 54us/step - loss: 0.1903 - acc: 0.9247 - val_loss: 0.2648 - val_acc: 0.8630\n",
      "Epoch 65/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.1845 - acc: 0.9274 - val_loss: 0.2760 - val_acc: 0.8568\n",
      "Epoch 66/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1855 - acc: 0.9262 - val_loss: 0.3030 - val_acc: 0.8402\n",
      "Epoch 67/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1910 - acc: 0.9244 - val_loss: 0.3038 - val_acc: 0.8402\n",
      "Epoch 68/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1851 - acc: 0.9264 - val_loss: 0.2561 - val_acc: 0.8644\n",
      "Epoch 69/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1826 - acc: 0.9277 - val_loss: 0.2673 - val_acc: 0.8625\n",
      "Epoch 70/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.1819 - acc: 0.9294 - val_loss: 0.2539 - val_acc: 0.8687\n",
      "Epoch 71/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1800 - acc: 0.9283 - val_loss: 0.2646 - val_acc: 0.8654\n",
      "Epoch 72/750\n",
      "13253/13253 [==============================] - 1s 60us/step - loss: 0.1771 - acc: 0.9316 - val_loss: 0.2923 - val_acc: 0.8601\n",
      "Epoch 73/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1793 - acc: 0.9304 - val_loss: 0.2679 - val_acc: 0.8597\n",
      "Epoch 74/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1793 - acc: 0.9291 - val_loss: 0.2540 - val_acc: 0.8649\n",
      "Epoch 75/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1741 - acc: 0.9316 - val_loss: 0.2606 - val_acc: 0.8639\n",
      "Epoch 76/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1745 - acc: 0.9315 - val_loss: 0.2544 - val_acc: 0.8677\n",
      "Epoch 77/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1718 - acc: 0.9328 - val_loss: 0.2608 - val_acc: 0.8654\n",
      "Epoch 78/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1701 - acc: 0.9324 - val_loss: 0.2767 - val_acc: 0.8592\n",
      "Epoch 79/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1736 - acc: 0.9310 - val_loss: 0.2535 - val_acc: 0.8711\n",
      "Epoch 80/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1763 - acc: 0.9325 - val_loss: 0.2711 - val_acc: 0.8630\n",
      "Epoch 81/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1760 - acc: 0.9305 - val_loss: 0.2590 - val_acc: 0.8696\n",
      "Epoch 82/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1720 - acc: 0.9314 - val_loss: 0.2570 - val_acc: 0.8706\n",
      "Epoch 83/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1676 - acc: 0.9349 - val_loss: 0.2716 - val_acc: 0.8673\n",
      "Epoch 84/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1805 - acc: 0.9285 - val_loss: 0.2586 - val_acc: 0.8677\n",
      "Epoch 85/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1693 - acc: 0.9350 - val_loss: 0.2485 - val_acc: 0.8716\n",
      "Epoch 86/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1690 - acc: 0.9322 - val_loss: 0.2593 - val_acc: 0.8692\n",
      "Epoch 87/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1650 - acc: 0.9374 - val_loss: 0.2433 - val_acc: 0.8739\n",
      "Epoch 88/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1716 - acc: 0.9313 - val_loss: 0.2487 - val_acc: 0.8758\n",
      "Epoch 89/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1634 - acc: 0.9338 - val_loss: 0.2311 - val_acc: 0.8868\n",
      "Epoch 90/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1704 - acc: 0.9314 - val_loss: 0.2331 - val_acc: 0.8853\n",
      "Epoch 91/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1657 - acc: 0.9325 - val_loss: 0.2591 - val_acc: 0.8739\n",
      "Epoch 92/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1668 - acc: 0.9357 - val_loss: 0.2574 - val_acc: 0.8754\n",
      "Epoch 93/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1667 - acc: 0.9359 - val_loss: 0.2566 - val_acc: 0.8682\n",
      "Epoch 94/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1663 - acc: 0.9359 - val_loss: 0.2360 - val_acc: 0.8806\n",
      "Epoch 95/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1631 - acc: 0.9353 - val_loss: 0.2587 - val_acc: 0.8735\n",
      "Epoch 96/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1664 - acc: 0.9367 - val_loss: 0.2400 - val_acc: 0.8887\n",
      "Epoch 97/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1647 - acc: 0.9344 - val_loss: 0.2726 - val_acc: 0.8725\n",
      "Epoch 98/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1613 - acc: 0.9371 - val_loss: 0.2285 - val_acc: 0.8844\n",
      "Epoch 99/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1646 - acc: 0.9362 - val_loss: 0.2673 - val_acc: 0.8601\n",
      "Epoch 100/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1621 - acc: 0.9371 - val_loss: 0.2181 - val_acc: 0.8930\n",
      "Epoch 101/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1627 - acc: 0.9371 - val_loss: 0.2656 - val_acc: 0.8677\n",
      "Epoch 102/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1595 - acc: 0.9387 - val_loss: 0.2591 - val_acc: 0.8763\n",
      "Epoch 103/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1604 - acc: 0.9367 - val_loss: 0.2372 - val_acc: 0.8811\n",
      "Epoch 104/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1574 - acc: 0.9373 - val_loss: 0.2765 - val_acc: 0.8677\n",
      "Epoch 105/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1620 - acc: 0.9370 - val_loss: 0.2561 - val_acc: 0.8706\n",
      "Epoch 106/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1568 - acc: 0.9361 - val_loss: 0.2294 - val_acc: 0.8834\n",
      "Epoch 107/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1561 - acc: 0.9380 - val_loss: 0.2792 - val_acc: 0.8687\n",
      "Epoch 108/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1593 - acc: 0.9375 - val_loss: 0.2359 - val_acc: 0.8834\n",
      "Epoch 109/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1570 - acc: 0.9393 - val_loss: 0.2749 - val_acc: 0.8677\n",
      "Epoch 110/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1624 - acc: 0.9370 - val_loss: 0.2317 - val_acc: 0.8834\n",
      "Epoch 111/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1583 - acc: 0.9396 - val_loss: 0.2499 - val_acc: 0.8820\n",
      "Epoch 112/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1476 - acc: 0.9411 - val_loss: 0.2504 - val_acc: 0.8811\n",
      "Epoch 113/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1615 - acc: 0.9374 - val_loss: 0.2466 - val_acc: 0.8792\n",
      "Epoch 114/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1524 - acc: 0.9402 - val_loss: 0.2466 - val_acc: 0.8849\n",
      "Epoch 115/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1615 - acc: 0.9367 - val_loss: 0.2434 - val_acc: 0.8863\n",
      "Epoch 116/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1543 - acc: 0.9396 - val_loss: 0.2528 - val_acc: 0.8730\n",
      "Epoch 117/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1542 - acc: 0.9418 - val_loss: 0.2248 - val_acc: 0.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1550 - acc: 0.9385 - val_loss: 0.2398 - val_acc: 0.8830\n",
      "Epoch 119/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1486 - acc: 0.9414 - val_loss: 0.2549 - val_acc: 0.8787\n",
      "Epoch 120/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1513 - acc: 0.9419 - val_loss: 0.2500 - val_acc: 0.8820\n",
      "Epoch 121/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1420 - acc: 0.9456 - val_loss: 0.2256 - val_acc: 0.8972\n",
      "Epoch 122/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1509 - acc: 0.9404 - val_loss: 0.2524 - val_acc: 0.8834\n",
      "Epoch 123/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1565 - acc: 0.9415 - val_loss: 0.2404 - val_acc: 0.8844\n",
      "Epoch 124/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1491 - acc: 0.9406 - val_loss: 0.2406 - val_acc: 0.8858\n",
      "Epoch 125/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1506 - acc: 0.9394 - val_loss: 0.2351 - val_acc: 0.8963\n",
      "Epoch 126/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1526 - acc: 0.9412 - val_loss: 0.2329 - val_acc: 0.8930\n",
      "Epoch 127/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1501 - acc: 0.9411 - val_loss: 0.2470 - val_acc: 0.8877\n",
      "Epoch 128/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.1468 - acc: 0.9430 - val_loss: 0.2261 - val_acc: 0.8996\n",
      "Epoch 129/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.1410 - acc: 0.9457 - val_loss: 0.2284 - val_acc: 0.8958\n",
      "Epoch 130/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1491 - acc: 0.9429 - val_loss: 0.2261 - val_acc: 0.8873\n",
      "Epoch 131/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1459 - acc: 0.9433 - val_loss: 0.2438 - val_acc: 0.8877\n",
      "Epoch 132/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1423 - acc: 0.9442 - val_loss: 0.2255 - val_acc: 0.8982\n",
      "Epoch 133/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1433 - acc: 0.9441 - val_loss: 0.2533 - val_acc: 0.8849\n",
      "Epoch 134/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1518 - acc: 0.9387 - val_loss: 0.2134 - val_acc: 0.8968\n",
      "Epoch 135/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1449 - acc: 0.9438 - val_loss: 0.2520 - val_acc: 0.8787\n",
      "Epoch 136/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1451 - acc: 0.9445 - val_loss: 0.2476 - val_acc: 0.8868\n",
      "Epoch 137/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1484 - acc: 0.9434 - val_loss: 0.2406 - val_acc: 0.8801\n",
      "Epoch 138/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1448 - acc: 0.9440 - val_loss: 0.2198 - val_acc: 0.8930\n",
      "Epoch 139/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1450 - acc: 0.9440 - val_loss: 0.2297 - val_acc: 0.8953\n",
      "Epoch 140/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1426 - acc: 0.9443 - val_loss: 0.2100 - val_acc: 0.9034\n",
      "Epoch 141/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1414 - acc: 0.9445 - val_loss: 0.2271 - val_acc: 0.8972\n",
      "Epoch 142/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1383 - acc: 0.9458 - val_loss: 0.2353 - val_acc: 0.8896\n",
      "Epoch 143/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1431 - acc: 0.9449 - val_loss: 0.2240 - val_acc: 0.8939\n",
      "Epoch 144/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1458 - acc: 0.9425 - val_loss: 0.2277 - val_acc: 0.8906\n",
      "Epoch 145/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1458 - acc: 0.9433 - val_loss: 0.2362 - val_acc: 0.8882\n",
      "Epoch 146/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1418 - acc: 0.9449 - val_loss: 0.2338 - val_acc: 0.8972\n",
      "Epoch 147/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1456 - acc: 0.9427 - val_loss: 0.2347 - val_acc: 0.8944\n",
      "Epoch 148/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1425 - acc: 0.9430 - val_loss: 0.2183 - val_acc: 0.8977\n",
      "Epoch 149/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1439 - acc: 0.9445 - val_loss: 0.2091 - val_acc: 0.9020\n",
      "Epoch 150/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1410 - acc: 0.9446 - val_loss: 0.2308 - val_acc: 0.8963\n",
      "Epoch 151/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1417 - acc: 0.9449 - val_loss: 0.2327 - val_acc: 0.9006\n",
      "Epoch 152/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1456 - acc: 0.9436 - val_loss: 0.2316 - val_acc: 0.8972\n",
      "Epoch 153/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1382 - acc: 0.9479 - val_loss: 0.2425 - val_acc: 0.8963\n",
      "Epoch 154/750\n",
      "13253/13253 [==============================] - 1s 59us/step - loss: 0.1451 - acc: 0.9448 - val_loss: 0.2246 - val_acc: 0.9001\n",
      "Epoch 155/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1336 - acc: 0.9497 - val_loss: 0.2296 - val_acc: 0.9006\n",
      "Epoch 156/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1432 - acc: 0.9445 - val_loss: 0.2278 - val_acc: 0.8949\n",
      "Epoch 157/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1392 - acc: 0.9464 - val_loss: 0.2523 - val_acc: 0.8944\n",
      "Epoch 158/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1415 - acc: 0.9446 - val_loss: 0.2125 - val_acc: 0.8996\n",
      "Epoch 159/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1356 - acc: 0.9473 - val_loss: 0.2496 - val_acc: 0.8915\n",
      "Epoch 160/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1360 - acc: 0.9460 - val_loss: 0.2078 - val_acc: 0.9087\n",
      "Epoch 161/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1354 - acc: 0.9470 - val_loss: 0.2195 - val_acc: 0.9063\n",
      "Epoch 162/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1330 - acc: 0.9471 - val_loss: 0.2188 - val_acc: 0.9039\n",
      "Epoch 163/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1444 - acc: 0.9436 - val_loss: 0.2219 - val_acc: 0.8996\n",
      "Epoch 164/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1405 - acc: 0.9442 - val_loss: 0.2320 - val_acc: 0.9039\n",
      "Epoch 165/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1317 - acc: 0.9479 - val_loss: 0.2299 - val_acc: 0.9058\n",
      "Epoch 166/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1359 - acc: 0.9478 - val_loss: 0.2212 - val_acc: 0.9020\n",
      "Epoch 167/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1371 - acc: 0.9452 - val_loss: 0.2154 - val_acc: 0.9020\n",
      "Epoch 168/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1358 - acc: 0.9461 - val_loss: 0.2027 - val_acc: 0.9058\n",
      "Epoch 169/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1394 - acc: 0.9472 - val_loss: 0.2140 - val_acc: 0.9006\n",
      "Epoch 170/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1278 - acc: 0.9505 - val_loss: 0.2213 - val_acc: 0.9006\n",
      "Epoch 171/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1320 - acc: 0.9493 - val_loss: 0.2169 - val_acc: 0.9039\n",
      "Epoch 172/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1425 - acc: 0.9455 - val_loss: 0.2042 - val_acc: 0.9039\n",
      "Epoch 173/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1360 - acc: 0.9472 - val_loss: 0.2167 - val_acc: 0.9044\n",
      "Epoch 174/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1357 - acc: 0.9466 - val_loss: 0.2133 - val_acc: 0.9053\n",
      "Epoch 175/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1269 - acc: 0.9485 - val_loss: 0.2049 - val_acc: 0.9091\n",
      "Epoch 176/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1428 - acc: 0.9454 - val_loss: 0.2018 - val_acc: 0.9087\n",
      "Epoch 177/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1280 - acc: 0.9504 - val_loss: 0.2276 - val_acc: 0.9015\n",
      "Epoch 178/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1325 - acc: 0.9485 - val_loss: 0.2176 - val_acc: 0.9063\n",
      "Epoch 179/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1350 - acc: 0.9462 - val_loss: 0.2305 - val_acc: 0.9044\n",
      "Epoch 180/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1367 - acc: 0.9459 - val_loss: 0.2006 - val_acc: 0.9153\n",
      "Epoch 181/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1323 - acc: 0.9488 - val_loss: 0.2147 - val_acc: 0.9058\n",
      "Epoch 182/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1405 - acc: 0.9439 - val_loss: 0.2149 - val_acc: 0.9039\n",
      "Epoch 183/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1319 - acc: 0.9497 - val_loss: 0.2263 - val_acc: 0.9006\n",
      "Epoch 184/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1354 - acc: 0.9468 - val_loss: 0.2262 - val_acc: 0.9029\n",
      "Epoch 185/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1343 - acc: 0.9481 - val_loss: 0.2088 - val_acc: 0.9049\n",
      "Epoch 186/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1281 - acc: 0.9516 - val_loss: 0.1878 - val_acc: 0.9167\n",
      "Epoch 187/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1326 - acc: 0.9492 - val_loss: 0.2050 - val_acc: 0.9129\n",
      "Epoch 188/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1304 - acc: 0.9499 - val_loss: 0.2055 - val_acc: 0.9082\n",
      "Epoch 189/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1277 - acc: 0.9482 - val_loss: 0.2575 - val_acc: 0.8958\n",
      "Epoch 190/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1349 - acc: 0.9488 - val_loss: 0.1928 - val_acc: 0.9077\n",
      "Epoch 191/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1340 - acc: 0.9493 - val_loss: 0.1971 - val_acc: 0.9120\n",
      "Epoch 192/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.1284 - acc: 0.9486 - val_loss: 0.2137 - val_acc: 0.9063\n",
      "Epoch 193/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1292 - acc: 0.9512 - val_loss: 0.2227 - val_acc: 0.9010\n",
      "Epoch 194/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1313 - acc: 0.9507 - val_loss: 0.2072 - val_acc: 0.9082\n",
      "Epoch 195/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1260 - acc: 0.9516 - val_loss: 0.2152 - val_acc: 0.9058\n",
      "Epoch 196/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1256 - acc: 0.9515 - val_loss: 0.2301 - val_acc: 0.9053\n",
      "Epoch 197/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1429 - acc: 0.9465 - val_loss: 0.2123 - val_acc: 0.9039\n",
      "Epoch 198/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1320 - acc: 0.9501 - val_loss: 0.2146 - val_acc: 0.9063\n",
      "Epoch 199/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1341 - acc: 0.9476 - val_loss: 0.2416 - val_acc: 0.8930\n",
      "Epoch 200/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1298 - acc: 0.9491 - val_loss: 0.2065 - val_acc: 0.9025\n",
      "Epoch 201/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1339 - acc: 0.9490 - val_loss: 0.2109 - val_acc: 0.9077\n",
      "Epoch 202/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1227 - acc: 0.9518 - val_loss: 0.2261 - val_acc: 0.9053\n",
      "Epoch 203/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1267 - acc: 0.9507 - val_loss: 0.2118 - val_acc: 0.9096\n",
      "Epoch 204/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1371 - acc: 0.9476 - val_loss: 0.2110 - val_acc: 0.9034\n",
      "Epoch 205/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1320 - acc: 0.9467 - val_loss: 0.2151 - val_acc: 0.9044\n",
      "Epoch 206/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1306 - acc: 0.9504 - val_loss: 0.2077 - val_acc: 0.9091\n",
      "Epoch 207/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1291 - acc: 0.9488 - val_loss: 0.2103 - val_acc: 0.9068\n",
      "Epoch 208/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1311 - acc: 0.9479 - val_loss: 0.2302 - val_acc: 0.8968\n",
      "Epoch 209/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1258 - acc: 0.9506 - val_loss: 0.1982 - val_acc: 0.9115\n",
      "Epoch 210/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1266 - acc: 0.9527 - val_loss: 0.2098 - val_acc: 0.9072\n",
      "Epoch 211/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1220 - acc: 0.9516 - val_loss: 0.2180 - val_acc: 0.9068\n",
      "Epoch 212/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1242 - acc: 0.9516 - val_loss: 0.2162 - val_acc: 0.9039\n",
      "Epoch 213/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1275 - acc: 0.9490 - val_loss: 0.2181 - val_acc: 0.9087\n",
      "Epoch 214/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1304 - acc: 0.9516 - val_loss: 0.2016 - val_acc: 0.9068\n",
      "Epoch 215/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1243 - acc: 0.9522 - val_loss: 0.2092 - val_acc: 0.9091\n",
      "Epoch 216/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1234 - acc: 0.9505 - val_loss: 0.2347 - val_acc: 0.9025\n",
      "Epoch 217/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1223 - acc: 0.9496 - val_loss: 0.2361 - val_acc: 0.9015\n",
      "Epoch 218/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1266 - acc: 0.9510 - val_loss: 0.2096 - val_acc: 0.9077\n",
      "Epoch 219/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1299 - acc: 0.9490 - val_loss: 0.2014 - val_acc: 0.9087\n",
      "Epoch 220/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1280 - acc: 0.9507 - val_loss: 0.2226 - val_acc: 0.9029\n",
      "Epoch 221/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1294 - acc: 0.9476 - val_loss: 0.2268 - val_acc: 0.9077\n",
      "Epoch 222/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1254 - acc: 0.9522 - val_loss: 0.2280 - val_acc: 0.9020\n",
      "Epoch 223/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1252 - acc: 0.9537 - val_loss: 0.2066 - val_acc: 0.9053\n",
      "Epoch 224/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1246 - acc: 0.9506 - val_loss: 0.1983 - val_acc: 0.9106\n",
      "Epoch 225/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1236 - acc: 0.9522 - val_loss: 0.2096 - val_acc: 0.9091\n",
      "Epoch 226/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1207 - acc: 0.9513 - val_loss: 0.1766 - val_acc: 0.9148\n",
      "Epoch 227/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1253 - acc: 0.9496 - val_loss: 0.2208 - val_acc: 0.8963\n",
      "Epoch 228/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1248 - acc: 0.9514 - val_loss: 0.2053 - val_acc: 0.9087\n",
      "Epoch 229/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1247 - acc: 0.9524 - val_loss: 0.2049 - val_acc: 0.9120\n",
      "Epoch 230/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.1207 - acc: 0.9521 - val_loss: 0.2068 - val_acc: 0.9063\n",
      "Epoch 231/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1302 - acc: 0.9491 - val_loss: 0.1932 - val_acc: 0.9148\n",
      "Epoch 232/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1205 - acc: 0.9517 - val_loss: 0.2091 - val_acc: 0.9106\n",
      "Epoch 233/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1225 - acc: 0.9515 - val_loss: 0.1951 - val_acc: 0.9115\n",
      "Epoch 234/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1234 - acc: 0.9513 - val_loss: 0.2028 - val_acc: 0.9158\n",
      "Epoch 235/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1242 - acc: 0.9525 - val_loss: 0.2027 - val_acc: 0.9134\n",
      "Epoch 236/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1238 - acc: 0.9525 - val_loss: 0.2025 - val_acc: 0.9134\n",
      "Epoch 237/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1250 - acc: 0.9504 - val_loss: 0.2322 - val_acc: 0.9058\n",
      "Epoch 238/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1237 - acc: 0.9523 - val_loss: 0.2247 - val_acc: 0.9091\n",
      "Epoch 239/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1180 - acc: 0.9547 - val_loss: 0.2077 - val_acc: 0.9129\n",
      "Epoch 240/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1251 - acc: 0.9510 - val_loss: 0.1911 - val_acc: 0.9139\n",
      "Epoch 241/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1248 - acc: 0.9519 - val_loss: 0.1701 - val_acc: 0.9220\n",
      "Epoch 242/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1196 - acc: 0.9510 - val_loss: 0.2054 - val_acc: 0.9115\n",
      "Epoch 243/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1289 - acc: 0.9494 - val_loss: 0.1914 - val_acc: 0.9172\n",
      "Epoch 244/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1199 - acc: 0.9543 - val_loss: 0.1914 - val_acc: 0.9077\n",
      "Epoch 245/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1198 - acc: 0.9531 - val_loss: 0.1808 - val_acc: 0.9225\n",
      "Epoch 246/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1254 - acc: 0.9515 - val_loss: 0.2152 - val_acc: 0.9125\n",
      "Epoch 247/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1176 - acc: 0.9527 - val_loss: 0.2143 - val_acc: 0.9129\n",
      "Epoch 248/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1184 - acc: 0.9542 - val_loss: 0.1941 - val_acc: 0.9144\n",
      "Epoch 249/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1202 - acc: 0.9528 - val_loss: 0.2214 - val_acc: 0.9082\n",
      "Epoch 250/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1203 - acc: 0.9522 - val_loss: 0.2122 - val_acc: 0.9144\n",
      "Epoch 251/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1221 - acc: 0.9516 - val_loss: 0.1938 - val_acc: 0.9158\n",
      "Epoch 252/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1214 - acc: 0.9512 - val_loss: 0.2086 - val_acc: 0.9096\n",
      "Epoch 253/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.1208 - acc: 0.9513 - val_loss: 0.2185 - val_acc: 0.9087\n",
      "Epoch 254/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1203 - acc: 0.9517 - val_loss: 0.2044 - val_acc: 0.9144\n",
      "Epoch 255/750\n",
      "13253/13253 [==============================] - 1s 61us/step - loss: 0.1238 - acc: 0.9520 - val_loss: 0.2139 - val_acc: 0.9129\n",
      "Epoch 256/750\n",
      "13253/13253 [==============================] - 1s 59us/step - loss: 0.1287 - acc: 0.9481 - val_loss: 0.2108 - val_acc: 0.9087\n",
      "Epoch 257/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1240 - acc: 0.9511 - val_loss: 0.1938 - val_acc: 0.9082\n",
      "Epoch 258/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.1211 - acc: 0.9540 - val_loss: 0.2163 - val_acc: 0.9144\n",
      "Epoch 259/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.1195 - acc: 0.9537 - val_loss: 0.2116 - val_acc: 0.9186\n",
      "Epoch 260/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.1176 - acc: 0.9558 - val_loss: 0.1943 - val_acc: 0.9148\n",
      "Epoch 261/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1227 - acc: 0.9508 - val_loss: 0.2022 - val_acc: 0.9087\n",
      "Epoch 262/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1202 - acc: 0.9541 - val_loss: 0.2100 - val_acc: 0.9110\n",
      "Epoch 263/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1191 - acc: 0.9531 - val_loss: 0.1951 - val_acc: 0.9129\n",
      "Epoch 264/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1150 - acc: 0.9545 - val_loss: 0.2024 - val_acc: 0.9163\n",
      "Epoch 265/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1198 - acc: 0.9525 - val_loss: 0.2167 - val_acc: 0.9053\n",
      "Epoch 266/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1246 - acc: 0.9519 - val_loss: 0.2105 - val_acc: 0.9139\n",
      "Epoch 267/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1208 - acc: 0.9526 - val_loss: 0.2091 - val_acc: 0.9144\n",
      "Epoch 268/750\n",
      "13253/13253 [==============================] - 1s 59us/step - loss: 0.1182 - acc: 0.9544 - val_loss: 0.1920 - val_acc: 0.9191\n",
      "Epoch 269/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1169 - acc: 0.9523 - val_loss: 0.2133 - val_acc: 0.9087\n",
      "Epoch 270/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1171 - acc: 0.9531 - val_loss: 0.1912 - val_acc: 0.9134\n",
      "Epoch 271/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.1195 - acc: 0.9540 - val_loss: 0.1793 - val_acc: 0.9206\n",
      "Epoch 272/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1217 - acc: 0.9516 - val_loss: 0.1875 - val_acc: 0.9144\n",
      "Epoch 273/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1172 - acc: 0.9539 - val_loss: 0.2063 - val_acc: 0.9120\n",
      "Epoch 274/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1125 - acc: 0.9554 - val_loss: 0.2095 - val_acc: 0.9148\n",
      "Epoch 275/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1187 - acc: 0.9532 - val_loss: 0.2123 - val_acc: 0.9072\n",
      "Epoch 276/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1157 - acc: 0.9535 - val_loss: 0.2118 - val_acc: 0.9158\n",
      "Epoch 277/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1124 - acc: 0.9575 - val_loss: 0.2050 - val_acc: 0.9153\n",
      "Epoch 278/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1222 - acc: 0.9513 - val_loss: 0.2038 - val_acc: 0.9120\n",
      "Epoch 279/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1169 - acc: 0.9546 - val_loss: 0.2084 - val_acc: 0.9129\n",
      "Epoch 280/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1197 - acc: 0.9534 - val_loss: 0.2074 - val_acc: 0.9115\n",
      "Epoch 281/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1223 - acc: 0.9523 - val_loss: 0.2082 - val_acc: 0.9096\n",
      "Epoch 282/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1185 - acc: 0.9526 - val_loss: 0.2173 - val_acc: 0.9101\n",
      "Epoch 283/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1176 - acc: 0.9519 - val_loss: 0.2089 - val_acc: 0.9068\n",
      "Epoch 284/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1205 - acc: 0.9522 - val_loss: 0.1911 - val_acc: 0.9167\n",
      "Epoch 285/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1126 - acc: 0.9549 - val_loss: 0.2070 - val_acc: 0.9125\n",
      "Epoch 286/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1183 - acc: 0.9558 - val_loss: 0.2089 - val_acc: 0.9106\n",
      "Epoch 287/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1168 - acc: 0.9533 - val_loss: 0.1988 - val_acc: 0.9139\n",
      "Epoch 288/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1173 - acc: 0.9555 - val_loss: 0.1886 - val_acc: 0.9220\n",
      "Epoch 289/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1207 - acc: 0.9531 - val_loss: 0.2200 - val_acc: 0.9120\n",
      "Epoch 290/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1146 - acc: 0.9538 - val_loss: 0.1964 - val_acc: 0.9177\n",
      "Epoch 291/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1183 - acc: 0.9550 - val_loss: 0.1986 - val_acc: 0.9158\n",
      "Epoch 292/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1182 - acc: 0.9546 - val_loss: 0.1788 - val_acc: 0.9244\n",
      "Epoch 293/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1131 - acc: 0.9545 - val_loss: 0.1936 - val_acc: 0.9167\n",
      "Epoch 294/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1203 - acc: 0.9542 - val_loss: 0.2079 - val_acc: 0.9148\n",
      "Epoch 295/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1212 - acc: 0.9532 - val_loss: 0.2151 - val_acc: 0.9082\n",
      "Epoch 296/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1095 - acc: 0.9577 - val_loss: 0.1880 - val_acc: 0.9248\n",
      "Epoch 297/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1137 - acc: 0.9549 - val_loss: 0.1958 - val_acc: 0.9177\n",
      "Epoch 298/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1164 - acc: 0.9531 - val_loss: 0.1855 - val_acc: 0.9220\n",
      "Epoch 299/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1216 - acc: 0.9536 - val_loss: 0.2168 - val_acc: 0.9082\n",
      "Epoch 300/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1120 - acc: 0.9565 - val_loss: 0.2013 - val_acc: 0.9144\n",
      "Epoch 301/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1066 - acc: 0.9577 - val_loss: 0.2135 - val_acc: 0.9120\n",
      "Epoch 302/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1120 - acc: 0.9569 - val_loss: 0.2043 - val_acc: 0.9191\n",
      "Epoch 303/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1177 - acc: 0.9551 - val_loss: 0.1872 - val_acc: 0.9201\n",
      "Epoch 304/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1110 - acc: 0.9566 - val_loss: 0.2075 - val_acc: 0.9096\n",
      "Epoch 305/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1136 - acc: 0.9559 - val_loss: 0.1883 - val_acc: 0.9158\n",
      "Epoch 306/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1154 - acc: 0.9562 - val_loss: 0.2052 - val_acc: 0.9082\n",
      "Epoch 307/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1140 - acc: 0.9553 - val_loss: 0.2070 - val_acc: 0.9148\n",
      "Epoch 308/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1170 - acc: 0.9537 - val_loss: 0.1925 - val_acc: 0.9191\n",
      "Epoch 309/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1102 - acc: 0.9554 - val_loss: 0.2163 - val_acc: 0.9077\n",
      "Epoch 310/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1189 - acc: 0.9553 - val_loss: 0.2023 - val_acc: 0.9163\n",
      "Epoch 311/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1124 - acc: 0.9548 - val_loss: 0.2036 - val_acc: 0.9144\n",
      "Epoch 312/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1101 - acc: 0.9559 - val_loss: 0.1889 - val_acc: 0.9191\n",
      "Epoch 313/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1122 - acc: 0.9548 - val_loss: 0.2202 - val_acc: 0.9120\n",
      "Epoch 314/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1152 - acc: 0.9535 - val_loss: 0.1862 - val_acc: 0.9206\n",
      "Epoch 315/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1158 - acc: 0.9546 - val_loss: 0.1963 - val_acc: 0.9153\n",
      "Epoch 316/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1140 - acc: 0.9534 - val_loss: 0.1976 - val_acc: 0.9167\n",
      "Epoch 317/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1053 - acc: 0.9582 - val_loss: 0.1970 - val_acc: 0.9220\n",
      "Epoch 318/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1115 - acc: 0.9571 - val_loss: 0.1944 - val_acc: 0.9186\n",
      "Epoch 319/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1140 - acc: 0.9542 - val_loss: 0.1805 - val_acc: 0.9206\n",
      "Epoch 320/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1119 - acc: 0.9574 - val_loss: 0.2080 - val_acc: 0.9115\n",
      "Epoch 321/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1101 - acc: 0.9557 - val_loss: 0.2021 - val_acc: 0.9153\n",
      "Epoch 322/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1053 - acc: 0.9571 - val_loss: 0.1829 - val_acc: 0.9229\n",
      "Epoch 323/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1151 - acc: 0.9539 - val_loss: 0.2048 - val_acc: 0.9134\n",
      "Epoch 324/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1133 - acc: 0.9541 - val_loss: 0.2006 - val_acc: 0.9144\n",
      "Epoch 325/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1102 - acc: 0.9563 - val_loss: 0.1925 - val_acc: 0.9186\n",
      "Epoch 326/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1116 - acc: 0.9552 - val_loss: 0.1959 - val_acc: 0.9153\n",
      "Epoch 327/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1099 - acc: 0.9581 - val_loss: 0.1788 - val_acc: 0.9201\n",
      "Epoch 328/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1150 - acc: 0.9559 - val_loss: 0.1890 - val_acc: 0.9201\n",
      "Epoch 329/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1088 - acc: 0.9577 - val_loss: 0.1883 - val_acc: 0.9206\n",
      "Epoch 330/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1110 - acc: 0.9564 - val_loss: 0.1973 - val_acc: 0.9186\n",
      "Epoch 331/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1144 - acc: 0.9556 - val_loss: 0.1984 - val_acc: 0.9144\n",
      "Epoch 332/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1113 - acc: 0.9535 - val_loss: 0.1800 - val_acc: 0.9191\n",
      "Epoch 333/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1144 - acc: 0.9556 - val_loss: 0.1864 - val_acc: 0.9201\n",
      "Epoch 334/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1111 - acc: 0.9570 - val_loss: 0.1935 - val_acc: 0.9177\n",
      "Epoch 335/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1102 - acc: 0.9559 - val_loss: 0.1868 - val_acc: 0.9239\n",
      "Epoch 336/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1112 - acc: 0.9564 - val_loss: 0.2055 - val_acc: 0.9144\n",
      "Epoch 337/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1071 - acc: 0.9585 - val_loss: 0.1918 - val_acc: 0.9191\n",
      "Epoch 338/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1158 - acc: 0.9547 - val_loss: 0.1837 - val_acc: 0.9239\n",
      "Epoch 339/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1094 - acc: 0.9558 - val_loss: 0.1838 - val_acc: 0.9253\n",
      "Epoch 340/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1172 - acc: 0.9550 - val_loss: 0.2000 - val_acc: 0.9158\n",
      "Epoch 341/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1098 - acc: 0.9561 - val_loss: 0.1862 - val_acc: 0.9177\n",
      "Epoch 342/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1108 - acc: 0.9565 - val_loss: 0.1806 - val_acc: 0.9258\n",
      "Epoch 343/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1046 - acc: 0.9581 - val_loss: 0.1797 - val_acc: 0.9206\n",
      "Epoch 344/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1012 - acc: 0.9594 - val_loss: 0.1858 - val_acc: 0.9225\n",
      "Epoch 345/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1124 - acc: 0.9568 - val_loss: 0.1959 - val_acc: 0.9186\n",
      "Epoch 346/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1064 - acc: 0.9577 - val_loss: 0.1927 - val_acc: 0.9177\n",
      "Epoch 347/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1117 - acc: 0.9587 - val_loss: 0.1853 - val_acc: 0.9172\n",
      "Epoch 348/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1130 - acc: 0.9562 - val_loss: 0.1916 - val_acc: 0.9201\n",
      "Epoch 349/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1142 - acc: 0.9560 - val_loss: 0.1888 - val_acc: 0.9239\n",
      "Epoch 350/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1084 - acc: 0.9574 - val_loss: 0.1805 - val_acc: 0.9220\n",
      "Epoch 351/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1130 - acc: 0.9576 - val_loss: 0.2044 - val_acc: 0.9167\n",
      "Epoch 352/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1168 - acc: 0.9545 - val_loss: 0.1916 - val_acc: 0.9153\n",
      "Epoch 353/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1130 - acc: 0.9539 - val_loss: 0.1793 - val_acc: 0.9220\n",
      "Epoch 354/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1086 - acc: 0.9584 - val_loss: 0.1999 - val_acc: 0.9206\n",
      "Epoch 355/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1100 - acc: 0.9577 - val_loss: 0.2070 - val_acc: 0.9158\n",
      "Epoch 356/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1106 - acc: 0.9575 - val_loss: 0.2020 - val_acc: 0.9172\n",
      "Epoch 357/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1050 - acc: 0.9584 - val_loss: 0.1970 - val_acc: 0.9196\n",
      "Epoch 358/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1117 - acc: 0.9558 - val_loss: 0.1768 - val_acc: 0.9239\n",
      "Epoch 359/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1121 - acc: 0.9565 - val_loss: 0.2091 - val_acc: 0.9139\n",
      "Epoch 360/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1075 - acc: 0.9574 - val_loss: 0.1955 - val_acc: 0.9253\n",
      "Epoch 361/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1030 - acc: 0.9599 - val_loss: 0.2135 - val_acc: 0.9163\n",
      "Epoch 362/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1075 - acc: 0.9572 - val_loss: 0.1948 - val_acc: 0.9229\n",
      "Epoch 363/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1166 - acc: 0.9536 - val_loss: 0.1847 - val_acc: 0.9201\n",
      "Epoch 364/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1103 - acc: 0.9566 - val_loss: 0.1754 - val_acc: 0.9248\n",
      "Epoch 365/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1085 - acc: 0.9577 - val_loss: 0.2005 - val_acc: 0.9172\n",
      "Epoch 366/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1101 - acc: 0.9552 - val_loss: 0.1896 - val_acc: 0.9234\n",
      "Epoch 367/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1118 - acc: 0.9555 - val_loss: 0.2029 - val_acc: 0.9144\n",
      "Epoch 368/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1088 - acc: 0.9595 - val_loss: 0.2013 - val_acc: 0.9167\n",
      "Epoch 369/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1092 - acc: 0.9566 - val_loss: 0.2034 - val_acc: 0.9120\n",
      "Epoch 370/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1090 - acc: 0.9565 - val_loss: 0.2226 - val_acc: 0.9106\n",
      "Epoch 371/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1080 - acc: 0.9567 - val_loss: 0.1814 - val_acc: 0.9291\n",
      "Epoch 372/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1051 - acc: 0.9579 - val_loss: 0.1950 - val_acc: 0.9244\n",
      "Epoch 373/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1169 - acc: 0.9559 - val_loss: 0.1914 - val_acc: 0.9196\n",
      "Epoch 374/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1115 - acc: 0.9559 - val_loss: 0.1945 - val_acc: 0.9215\n",
      "Epoch 375/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1081 - acc: 0.9565 - val_loss: 0.1831 - val_acc: 0.9282\n",
      "Epoch 376/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1110 - acc: 0.9570 - val_loss: 0.1998 - val_acc: 0.9206\n",
      "Epoch 377/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1117 - acc: 0.9565 - val_loss: 0.1878 - val_acc: 0.9229\n",
      "Epoch 378/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1045 - acc: 0.9591 - val_loss: 0.1893 - val_acc: 0.9229\n",
      "Epoch 379/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1053 - acc: 0.9571 - val_loss: 0.2073 - val_acc: 0.9201\n",
      "Epoch 380/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1056 - acc: 0.9583 - val_loss: 0.1838 - val_acc: 0.9253\n",
      "Epoch 381/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1080 - acc: 0.9575 - val_loss: 0.2014 - val_acc: 0.9210\n",
      "Epoch 382/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1079 - acc: 0.9585 - val_loss: 0.1898 - val_acc: 0.9267\n",
      "Epoch 383/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1044 - acc: 0.9608 - val_loss: 0.1989 - val_acc: 0.9182\n",
      "Epoch 384/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1089 - acc: 0.9574 - val_loss: 0.1834 - val_acc: 0.9263\n",
      "Epoch 385/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1075 - acc: 0.9583 - val_loss: 0.1954 - val_acc: 0.9244\n",
      "Epoch 386/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1042 - acc: 0.9570 - val_loss: 0.2049 - val_acc: 0.9206\n",
      "Epoch 387/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1030 - acc: 0.9590 - val_loss: 0.2134 - val_acc: 0.9158\n",
      "Epoch 388/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1115 - acc: 0.9562 - val_loss: 0.1803 - val_acc: 0.9263\n",
      "Epoch 389/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1141 - acc: 0.9547 - val_loss: 0.2046 - val_acc: 0.9196\n",
      "Epoch 390/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1108 - acc: 0.9571 - val_loss: 0.2044 - val_acc: 0.9196\n",
      "Epoch 391/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1084 - acc: 0.9584 - val_loss: 0.1928 - val_acc: 0.9220\n",
      "Epoch 392/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1071 - acc: 0.9571 - val_loss: 0.1910 - val_acc: 0.9229\n",
      "Epoch 393/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1099 - acc: 0.9554 - val_loss: 0.2030 - val_acc: 0.9177\n",
      "Epoch 394/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1065 - acc: 0.9573 - val_loss: 0.1963 - val_acc: 0.9229\n",
      "Epoch 395/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1021 - acc: 0.9590 - val_loss: 0.2120 - val_acc: 0.9115\n",
      "Epoch 396/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1063 - acc: 0.9595 - val_loss: 0.2072 - val_acc: 0.9167\n",
      "Epoch 397/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1022 - acc: 0.9595 - val_loss: 0.1840 - val_acc: 0.9248\n",
      "Epoch 398/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1090 - acc: 0.9568 - val_loss: 0.2077 - val_acc: 0.9182\n",
      "Epoch 399/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1064 - acc: 0.9568 - val_loss: 0.1861 - val_acc: 0.9277\n",
      "Epoch 400/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1073 - acc: 0.9587 - val_loss: 0.1827 - val_acc: 0.9239\n",
      "Epoch 401/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1077 - acc: 0.9577 - val_loss: 0.1756 - val_acc: 0.9305\n",
      "Epoch 402/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1067 - acc: 0.9571 - val_loss: 0.1803 - val_acc: 0.9263\n",
      "Epoch 403/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1082 - acc: 0.9575 - val_loss: 0.2055 - val_acc: 0.9225\n",
      "Epoch 404/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1081 - acc: 0.9568 - val_loss: 0.2169 - val_acc: 0.9115\n",
      "Epoch 405/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1172 - acc: 0.9562 - val_loss: 0.1845 - val_acc: 0.9220\n",
      "Epoch 406/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1060 - acc: 0.9604 - val_loss: 0.1909 - val_acc: 0.9253\n",
      "Epoch 407/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1073 - acc: 0.9591 - val_loss: 0.2057 - val_acc: 0.9248\n",
      "Epoch 408/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1025 - acc: 0.9599 - val_loss: 0.2091 - val_acc: 0.9248\n",
      "Epoch 409/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1076 - acc: 0.9584 - val_loss: 0.2054 - val_acc: 0.9258\n",
      "Epoch 410/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1043 - acc: 0.9574 - val_loss: 0.1901 - val_acc: 0.9225\n",
      "Epoch 411/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1053 - acc: 0.9582 - val_loss: 0.1816 - val_acc: 0.9267\n",
      "Epoch 412/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1024 - acc: 0.9601 - val_loss: 0.1779 - val_acc: 0.9272\n",
      "Epoch 413/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1047 - acc: 0.9597 - val_loss: 0.1925 - val_acc: 0.9210\n",
      "Epoch 414/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1047 - acc: 0.9593 - val_loss: 0.2229 - val_acc: 0.9148\n",
      "Epoch 415/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1062 - acc: 0.9575 - val_loss: 0.2103 - val_acc: 0.9234\n",
      "Epoch 416/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1094 - acc: 0.9548 - val_loss: 0.2037 - val_acc: 0.9172\n",
      "Epoch 417/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1058 - acc: 0.9575 - val_loss: 0.1930 - val_acc: 0.9206\n",
      "Epoch 418/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1047 - acc: 0.9574 - val_loss: 0.2061 - val_acc: 0.9182\n",
      "Epoch 419/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1035 - acc: 0.9596 - val_loss: 0.2105 - val_acc: 0.9186\n",
      "Epoch 420/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1029 - acc: 0.9568 - val_loss: 0.2050 - val_acc: 0.9163\n",
      "Epoch 421/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1089 - acc: 0.9562 - val_loss: 0.2023 - val_acc: 0.9253\n",
      "Epoch 422/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1090 - acc: 0.9571 - val_loss: 0.2022 - val_acc: 0.9196\n",
      "Epoch 423/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1052 - acc: 0.9597 - val_loss: 0.1963 - val_acc: 0.9234\n",
      "Epoch 424/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1057 - acc: 0.9603 - val_loss: 0.1861 - val_acc: 0.9225\n",
      "Epoch 425/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1035 - acc: 0.9599 - val_loss: 0.1804 - val_acc: 0.9210\n",
      "Epoch 426/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1034 - acc: 0.9584 - val_loss: 0.1788 - val_acc: 0.9263\n",
      "Epoch 427/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1059 - acc: 0.9569 - val_loss: 0.1924 - val_acc: 0.9267\n",
      "Epoch 428/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1015 - acc: 0.9604 - val_loss: 0.1860 - val_acc: 0.9291\n",
      "Epoch 429/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1055 - acc: 0.9596 - val_loss: 0.1903 - val_acc: 0.9229\n",
      "Epoch 430/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1111 - acc: 0.9585 - val_loss: 0.1836 - val_acc: 0.9272\n",
      "Epoch 431/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1014 - acc: 0.9591 - val_loss: 0.1777 - val_acc: 0.9282\n",
      "Epoch 432/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0998 - acc: 0.9593 - val_loss: 0.1838 - val_acc: 0.9263\n",
      "Epoch 433/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.1098 - acc: 0.9562 - val_loss: 0.1999 - val_acc: 0.9201\n",
      "Epoch 434/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1017 - acc: 0.9600 - val_loss: 0.1913 - val_acc: 0.9258\n",
      "Epoch 435/750\n",
      "13253/13253 [==============================] - 1s 63us/step - loss: 0.1005 - acc: 0.9598 - val_loss: 0.1967 - val_acc: 0.9263\n",
      "Epoch 436/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.1076 - acc: 0.9578 - val_loss: 0.1890 - val_acc: 0.9253\n",
      "Epoch 437/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1025 - acc: 0.9584 - val_loss: 0.1812 - val_acc: 0.9301\n",
      "Epoch 438/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1072 - acc: 0.9586 - val_loss: 0.1841 - val_acc: 0.9253\n",
      "Epoch 439/750\n",
      "13253/13253 [==============================] - 1s 54us/step - loss: 0.1077 - acc: 0.9581 - val_loss: 0.1999 - val_acc: 0.9210\n",
      "Epoch 440/750\n",
      "13253/13253 [==============================] - 1s 54us/step - loss: 0.1064 - acc: 0.9590 - val_loss: 0.1918 - val_acc: 0.9248\n",
      "Epoch 441/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1071 - acc: 0.9571 - val_loss: 0.1990 - val_acc: 0.9234\n",
      "Epoch 442/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1006 - acc: 0.9590 - val_loss: 0.1922 - val_acc: 0.9239\n",
      "Epoch 443/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.1048 - acc: 0.9584 - val_loss: 0.2067 - val_acc: 0.9206\n",
      "Epoch 444/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1033 - acc: 0.9576 - val_loss: 0.2033 - val_acc: 0.9215\n",
      "Epoch 445/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1096 - acc: 0.9571 - val_loss: 0.2062 - val_acc: 0.9206\n",
      "Epoch 446/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.1068 - acc: 0.9573 - val_loss: 0.1860 - val_acc: 0.9267\n",
      "Epoch 447/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1038 - acc: 0.9602 - val_loss: 0.2027 - val_acc: 0.9229\n",
      "Epoch 448/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1059 - acc: 0.9582 - val_loss: 0.1927 - val_acc: 0.9258\n",
      "Epoch 449/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1062 - acc: 0.9593 - val_loss: 0.1974 - val_acc: 0.9244\n",
      "Epoch 450/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1035 - acc: 0.9592 - val_loss: 0.1928 - val_acc: 0.9234\n",
      "Epoch 451/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1040 - acc: 0.9571 - val_loss: 0.2066 - val_acc: 0.9158\n",
      "Epoch 452/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1036 - acc: 0.9582 - val_loss: 0.2144 - val_acc: 0.9244\n",
      "Epoch 453/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1044 - acc: 0.9589 - val_loss: 0.1877 - val_acc: 0.9286\n",
      "Epoch 454/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0994 - acc: 0.9604 - val_loss: 0.1918 - val_acc: 0.9248\n",
      "Epoch 455/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1026 - acc: 0.9592 - val_loss: 0.1825 - val_acc: 0.9272\n",
      "Epoch 456/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1068 - acc: 0.9573 - val_loss: 0.1807 - val_acc: 0.9286\n",
      "Epoch 457/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1042 - acc: 0.9579 - val_loss: 0.1858 - val_acc: 0.9263\n",
      "Epoch 458/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1027 - acc: 0.9590 - val_loss: 0.1758 - val_acc: 0.9296\n",
      "Epoch 459/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1016 - acc: 0.9599 - val_loss: 0.1949 - val_acc: 0.9244\n",
      "Epoch 460/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1023 - acc: 0.9596 - val_loss: 0.1848 - val_acc: 0.9296\n",
      "Epoch 461/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1038 - acc: 0.9584 - val_loss: 0.2017 - val_acc: 0.9234\n",
      "Epoch 462/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1122 - acc: 0.9546 - val_loss: 0.1918 - val_acc: 0.9277\n",
      "Epoch 463/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1083 - acc: 0.9563 - val_loss: 0.1940 - val_acc: 0.9272\n",
      "Epoch 464/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1066 - acc: 0.9571 - val_loss: 0.1926 - val_acc: 0.9248\n",
      "Epoch 465/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1045 - acc: 0.9597 - val_loss: 0.1890 - val_acc: 0.9244\n",
      "Epoch 466/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0962 - acc: 0.9631 - val_loss: 0.1821 - val_acc: 0.9329\n",
      "Epoch 467/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1000 - acc: 0.9588 - val_loss: 0.1923 - val_acc: 0.9263\n",
      "Epoch 468/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1044 - acc: 0.9585 - val_loss: 0.1890 - val_acc: 0.9244\n",
      "Epoch 469/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1011 - acc: 0.9596 - val_loss: 0.1800 - val_acc: 0.9253\n",
      "Epoch 470/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1074 - acc: 0.9585 - val_loss: 0.1919 - val_acc: 0.9229\n",
      "Epoch 471/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1012 - acc: 0.9596 - val_loss: 0.1935 - val_acc: 0.9239\n",
      "Epoch 472/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1049 - acc: 0.9575 - val_loss: 0.2008 - val_acc: 0.9253\n",
      "Epoch 473/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1055 - acc: 0.9587 - val_loss: 0.1851 - val_acc: 0.9239\n",
      "Epoch 474/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1034 - acc: 0.9591 - val_loss: 0.2024 - val_acc: 0.9196\n",
      "Epoch 475/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1041 - acc: 0.9578 - val_loss: 0.1945 - val_acc: 0.9229\n",
      "Epoch 476/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1047 - acc: 0.9592 - val_loss: 0.1751 - val_acc: 0.9267\n",
      "Epoch 477/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1013 - acc: 0.9597 - val_loss: 0.1918 - val_acc: 0.9229\n",
      "Epoch 478/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1042 - acc: 0.9571 - val_loss: 0.1799 - val_acc: 0.9267\n",
      "Epoch 479/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.1021 - acc: 0.9588 - val_loss: 0.1926 - val_acc: 0.9210\n",
      "Epoch 480/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1042 - acc: 0.9611 - val_loss: 0.1808 - val_acc: 0.9277\n",
      "Epoch 481/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1005 - acc: 0.9602 - val_loss: 0.2004 - val_acc: 0.9210\n",
      "Epoch 482/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0969 - acc: 0.9613 - val_loss: 0.1867 - val_acc: 0.9220\n",
      "Epoch 483/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1054 - acc: 0.9580 - val_loss: 0.1739 - val_acc: 0.9286\n",
      "Epoch 484/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0992 - acc: 0.9602 - val_loss: 0.2046 - val_acc: 0.9196\n",
      "Epoch 485/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1040 - acc: 0.9591 - val_loss: 0.1986 - val_acc: 0.9239\n",
      "Epoch 486/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0970 - acc: 0.9623 - val_loss: 0.1862 - val_acc: 0.9263\n",
      "Epoch 487/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1046 - acc: 0.9598 - val_loss: 0.1675 - val_acc: 0.9272\n",
      "Epoch 488/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1005 - acc: 0.9594 - val_loss: 0.1805 - val_acc: 0.9263\n",
      "Epoch 489/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1033 - acc: 0.9587 - val_loss: 0.2065 - val_acc: 0.9182\n",
      "Epoch 490/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1020 - acc: 0.9583 - val_loss: 0.1675 - val_acc: 0.9263\n",
      "Epoch 491/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1012 - acc: 0.9623 - val_loss: 0.1865 - val_acc: 0.9229\n",
      "Epoch 492/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1045 - acc: 0.9585 - val_loss: 0.1767 - val_acc: 0.9282\n",
      "Epoch 493/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1062 - acc: 0.9591 - val_loss: 0.1722 - val_acc: 0.9301\n",
      "Epoch 494/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1027 - acc: 0.9587 - val_loss: 0.1927 - val_acc: 0.9210\n",
      "Epoch 495/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1017 - acc: 0.9600 - val_loss: 0.1691 - val_acc: 0.9305\n",
      "Epoch 496/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1015 - acc: 0.9608 - val_loss: 0.1821 - val_acc: 0.9229\n",
      "Epoch 497/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1014 - acc: 0.9611 - val_loss: 0.1648 - val_acc: 0.9353\n",
      "Epoch 498/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0972 - acc: 0.9599 - val_loss: 0.1834 - val_acc: 0.9234\n",
      "Epoch 499/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0993 - acc: 0.9590 - val_loss: 0.1986 - val_acc: 0.9196\n",
      "Epoch 500/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1027 - acc: 0.9605 - val_loss: 0.1888 - val_acc: 0.9253\n",
      "Epoch 501/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0979 - acc: 0.9620 - val_loss: 0.1860 - val_acc: 0.9248\n",
      "Epoch 502/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1029 - acc: 0.9602 - val_loss: 0.1884 - val_acc: 0.9229\n",
      "Epoch 503/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1002 - acc: 0.9599 - val_loss: 0.1855 - val_acc: 0.9286\n",
      "Epoch 504/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0996 - acc: 0.9614 - val_loss: 0.1819 - val_acc: 0.9286\n",
      "Epoch 505/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0960 - acc: 0.9630 - val_loss: 0.1806 - val_acc: 0.9215\n",
      "Epoch 506/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0975 - acc: 0.9595 - val_loss: 0.1936 - val_acc: 0.9225\n",
      "Epoch 507/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1013 - acc: 0.9595 - val_loss: 0.1848 - val_acc: 0.9225\n",
      "Epoch 508/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1050 - acc: 0.9594 - val_loss: 0.2025 - val_acc: 0.9225\n",
      "Epoch 509/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.1034 - acc: 0.9589 - val_loss: 0.1936 - val_acc: 0.9244\n",
      "Epoch 510/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.1007 - acc: 0.9608 - val_loss: 0.1744 - val_acc: 0.9244\n",
      "Epoch 511/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1000 - acc: 0.9607 - val_loss: 0.1827 - val_acc: 0.9244\n",
      "Epoch 512/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.1018 - acc: 0.9611 - val_loss: 0.1761 - val_acc: 0.9291\n",
      "Epoch 513/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.0985 - acc: 0.9612 - val_loss: 0.2008 - val_acc: 0.9244\n",
      "Epoch 514/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.1003 - acc: 0.9616 - val_loss: 0.1899 - val_acc: 0.9272\n",
      "Epoch 515/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.1081 - acc: 0.9568 - val_loss: 0.1991 - val_acc: 0.9258\n",
      "Epoch 516/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1082 - acc: 0.9587 - val_loss: 0.1776 - val_acc: 0.9244\n",
      "Epoch 517/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1025 - acc: 0.9599 - val_loss: 0.1960 - val_acc: 0.9215\n",
      "Epoch 518/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0970 - acc: 0.9627 - val_loss: 0.2015 - val_acc: 0.9229\n",
      "Epoch 519/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0961 - acc: 0.9619 - val_loss: 0.2001 - val_acc: 0.9239\n",
      "Epoch 520/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1020 - acc: 0.9593 - val_loss: 0.2004 - val_acc: 0.9239\n",
      "Epoch 521/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1021 - acc: 0.9604 - val_loss: 0.1860 - val_acc: 0.9229\n",
      "Epoch 522/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0990 - acc: 0.9614 - val_loss: 0.1773 - val_acc: 0.9263\n",
      "Epoch 523/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1027 - acc: 0.9586 - val_loss: 0.1850 - val_acc: 0.9253\n",
      "Epoch 524/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0995 - acc: 0.9609 - val_loss: 0.1869 - val_acc: 0.9248\n",
      "Epoch 525/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0981 - acc: 0.9606 - val_loss: 0.1777 - val_acc: 0.9291\n",
      "Epoch 526/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0941 - acc: 0.9614 - val_loss: 0.1762 - val_acc: 0.9282\n",
      "Epoch 527/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1007 - acc: 0.9608 - val_loss: 0.1916 - val_acc: 0.9244\n",
      "Epoch 528/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0986 - acc: 0.9618 - val_loss: 0.1891 - val_acc: 0.9296\n",
      "Epoch 529/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.1005 - acc: 0.9608 - val_loss: 0.1877 - val_acc: 0.9310\n",
      "Epoch 530/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1030 - acc: 0.9592 - val_loss: 0.1848 - val_acc: 0.9291\n",
      "Epoch 531/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0955 - acc: 0.9613 - val_loss: 0.1914 - val_acc: 0.9263\n",
      "Epoch 532/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1041 - acc: 0.9613 - val_loss: 0.1947 - val_acc: 0.9225\n",
      "Epoch 533/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1032 - acc: 0.9596 - val_loss: 0.1725 - val_acc: 0.9258\n",
      "Epoch 534/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0969 - acc: 0.9617 - val_loss: 0.1846 - val_acc: 0.9277\n",
      "Epoch 535/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0994 - acc: 0.9614 - val_loss: 0.2035 - val_acc: 0.9206\n",
      "Epoch 536/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0977 - acc: 0.9620 - val_loss: 0.1844 - val_acc: 0.9263\n",
      "Epoch 537/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.1016 - acc: 0.9580 - val_loss: 0.1864 - val_acc: 0.9301\n",
      "Epoch 538/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0974 - acc: 0.9595 - val_loss: 0.1768 - val_acc: 0.9324\n",
      "Epoch 539/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1027 - acc: 0.9596 - val_loss: 0.1870 - val_acc: 0.9229\n",
      "Epoch 540/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0980 - acc: 0.9620 - val_loss: 0.1657 - val_acc: 0.9353\n",
      "Epoch 541/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1026 - acc: 0.9597 - val_loss: 0.1812 - val_acc: 0.9220\n",
      "Epoch 542/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0954 - acc: 0.9611 - val_loss: 0.1949 - val_acc: 0.9234\n",
      "Epoch 543/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0987 - acc: 0.9620 - val_loss: 0.1742 - val_acc: 0.9296\n",
      "Epoch 544/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1013 - acc: 0.9591 - val_loss: 0.1957 - val_acc: 0.9215\n",
      "Epoch 545/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.1052 - acc: 0.9586 - val_loss: 0.1865 - val_acc: 0.9210\n",
      "Epoch 546/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0966 - acc: 0.9618 - val_loss: 0.1871 - val_acc: 0.9263\n",
      "Epoch 547/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0927 - acc: 0.9623 - val_loss: 0.1982 - val_acc: 0.9258\n",
      "Epoch 548/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1043 - acc: 0.9571 - val_loss: 0.2027 - val_acc: 0.9201\n",
      "Epoch 549/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0975 - acc: 0.9621 - val_loss: 0.1971 - val_acc: 0.9248\n",
      "Epoch 550/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0953 - acc: 0.9623 - val_loss: 0.1866 - val_acc: 0.9253\n",
      "Epoch 551/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.0969 - acc: 0.9605 - val_loss: 0.1935 - val_acc: 0.9253\n",
      "Epoch 552/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.1011 - acc: 0.9577 - val_loss: 0.1689 - val_acc: 0.9343\n",
      "Epoch 553/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.0985 - acc: 0.9611 - val_loss: 0.1644 - val_acc: 0.9348\n",
      "Epoch 554/750\n",
      "13253/13253 [==============================] - 1s 64us/step - loss: 0.0990 - acc: 0.9599 - val_loss: 0.1916 - val_acc: 0.9286\n",
      "Epoch 555/750\n",
      "13253/13253 [==============================] - 1s 58us/step - loss: 0.0971 - acc: 0.9620 - val_loss: 0.1672 - val_acc: 0.9301\n",
      "Epoch 556/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0979 - acc: 0.9607 - val_loss: 0.1907 - val_acc: 0.9248\n",
      "Epoch 557/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1014 - acc: 0.9602 - val_loss: 0.1781 - val_acc: 0.9272\n",
      "Epoch 558/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.1034 - acc: 0.9593 - val_loss: 0.1823 - val_acc: 0.9215\n",
      "Epoch 559/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.1084 - acc: 0.9581 - val_loss: 0.1894 - val_acc: 0.9229\n",
      "Epoch 560/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0968 - acc: 0.9620 - val_loss: 0.1773 - val_acc: 0.9277\n",
      "Epoch 561/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0995 - acc: 0.9608 - val_loss: 0.1891 - val_acc: 0.9253\n",
      "Epoch 562/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1012 - acc: 0.9622 - val_loss: 0.1855 - val_acc: 0.9248\n",
      "Epoch 563/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.1007 - acc: 0.9617 - val_loss: 0.1707 - val_acc: 0.9286\n",
      "Epoch 564/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0967 - acc: 0.9618 - val_loss: 0.1797 - val_acc: 0.9324\n",
      "Epoch 565/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0993 - acc: 0.9595 - val_loss: 0.1673 - val_acc: 0.9310\n",
      "Epoch 566/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0948 - acc: 0.9611 - val_loss: 0.1728 - val_acc: 0.9301\n",
      "Epoch 567/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0987 - acc: 0.9602 - val_loss: 0.1870 - val_acc: 0.9239\n",
      "Epoch 568/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0948 - acc: 0.9622 - val_loss: 0.1961 - val_acc: 0.9234\n",
      "Epoch 569/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0979 - acc: 0.9614 - val_loss: 0.1790 - val_acc: 0.9296\n",
      "Epoch 570/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0970 - acc: 0.9606 - val_loss: 0.1848 - val_acc: 0.9282\n",
      "Epoch 571/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0986 - acc: 0.9603 - val_loss: 0.1687 - val_acc: 0.9310\n",
      "Epoch 572/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0988 - acc: 0.9611 - val_loss: 0.1687 - val_acc: 0.9363\n",
      "Epoch 573/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0988 - acc: 0.9607 - val_loss: 0.2018 - val_acc: 0.9239\n",
      "Epoch 574/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0963 - acc: 0.9609 - val_loss: 0.1811 - val_acc: 0.9320\n",
      "Epoch 575/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0984 - acc: 0.9593 - val_loss: 0.2007 - val_acc: 0.9272\n",
      "Epoch 576/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0978 - acc: 0.9620 - val_loss: 0.1845 - val_acc: 0.9286\n",
      "Epoch 577/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0994 - acc: 0.9599 - val_loss: 0.1854 - val_acc: 0.9301\n",
      "Epoch 578/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0963 - acc: 0.9625 - val_loss: 0.1964 - val_acc: 0.9286\n",
      "Epoch 579/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0949 - acc: 0.9633 - val_loss: 0.2032 - val_acc: 0.9258\n",
      "Epoch 580/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0996 - acc: 0.9594 - val_loss: 0.1922 - val_acc: 0.9310\n",
      "Epoch 581/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0997 - acc: 0.9611 - val_loss: 0.1770 - val_acc: 0.9363\n",
      "Epoch 582/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0931 - acc: 0.9624 - val_loss: 0.1691 - val_acc: 0.9343\n",
      "Epoch 583/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0955 - acc: 0.9633 - val_loss: 0.1923 - val_acc: 0.9277\n",
      "Epoch 584/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0973 - acc: 0.9591 - val_loss: 0.1870 - val_acc: 0.9334\n",
      "Epoch 585/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0953 - acc: 0.9636 - val_loss: 0.1800 - val_acc: 0.9320\n",
      "Epoch 586/750\n",
      "13253/13253 [==============================] - 0s 38us/step - loss: 0.1000 - acc: 0.9610 - val_loss: 0.1814 - val_acc: 0.9291\n",
      "Epoch 587/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0952 - acc: 0.9631 - val_loss: 0.2152 - val_acc: 0.9139\n",
      "Epoch 588/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0958 - acc: 0.9608 - val_loss: 0.1842 - val_acc: 0.9272\n",
      "Epoch 589/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0961 - acc: 0.9613 - val_loss: 0.1869 - val_acc: 0.9272\n",
      "Epoch 590/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0928 - acc: 0.9629 - val_loss: 0.1811 - val_acc: 0.9267\n",
      "Epoch 591/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0969 - acc: 0.9620 - val_loss: 0.1808 - val_acc: 0.9305\n",
      "Epoch 592/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0974 - acc: 0.9620 - val_loss: 0.1947 - val_acc: 0.9277\n",
      "Epoch 593/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0997 - acc: 0.9619 - val_loss: 0.1943 - val_acc: 0.9225\n",
      "Epoch 594/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0974 - acc: 0.9617 - val_loss: 0.1838 - val_acc: 0.9263\n",
      "Epoch 595/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0937 - acc: 0.9639 - val_loss: 0.1838 - val_acc: 0.9282\n",
      "Epoch 596/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0971 - acc: 0.9622 - val_loss: 0.1692 - val_acc: 0.9291\n",
      "Epoch 597/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0948 - acc: 0.9631 - val_loss: 0.1704 - val_acc: 0.9348\n",
      "Epoch 598/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.1022 - acc: 0.9597 - val_loss: 0.1794 - val_acc: 0.9343\n",
      "Epoch 599/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0973 - acc: 0.9631 - val_loss: 0.1760 - val_acc: 0.9315\n",
      "Epoch 600/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0980 - acc: 0.9614 - val_loss: 0.1855 - val_acc: 0.9272\n",
      "Epoch 601/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0958 - acc: 0.9614 - val_loss: 0.1872 - val_acc: 0.9267\n",
      "Epoch 602/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.0980 - acc: 0.9629 - val_loss: 0.1853 - val_acc: 0.9244\n",
      "Epoch 603/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.0980 - acc: 0.9614 - val_loss: 0.1917 - val_acc: 0.9244\n",
      "Epoch 604/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0945 - acc: 0.9636 - val_loss: 0.1872 - val_acc: 0.9282\n",
      "Epoch 605/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0940 - acc: 0.9633 - val_loss: 0.1818 - val_acc: 0.9263\n",
      "Epoch 606/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.0892 - acc: 0.9654 - val_loss: 0.1902 - val_acc: 0.9263\n",
      "Epoch 607/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.1025 - acc: 0.9579 - val_loss: 0.1970 - val_acc: 0.9234\n",
      "Epoch 608/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0953 - acc: 0.9620 - val_loss: 0.1782 - val_acc: 0.9248\n",
      "Epoch 609/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0961 - acc: 0.9609 - val_loss: 0.2051 - val_acc: 0.9253\n",
      "Epoch 610/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0964 - acc: 0.9630 - val_loss: 0.1834 - val_acc: 0.9258\n",
      "Epoch 611/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0993 - acc: 0.9624 - val_loss: 0.1922 - val_acc: 0.9210\n",
      "Epoch 612/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0938 - acc: 0.9633 - val_loss: 0.1773 - val_acc: 0.9282\n",
      "Epoch 613/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.1065 - acc: 0.9596 - val_loss: 0.1790 - val_acc: 0.9267\n",
      "Epoch 614/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0909 - acc: 0.9644 - val_loss: 0.1716 - val_acc: 0.9334\n",
      "Epoch 615/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0960 - acc: 0.9623 - val_loss: 0.1686 - val_acc: 0.9339\n",
      "Epoch 616/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0971 - acc: 0.9608 - val_loss: 0.1622 - val_acc: 0.9305\n",
      "Epoch 617/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0946 - acc: 0.9631 - val_loss: 0.1818 - val_acc: 0.9301\n",
      "Epoch 618/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0939 - acc: 0.9632 - val_loss: 0.1851 - val_acc: 0.9253\n",
      "Epoch 619/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0943 - acc: 0.9620 - val_loss: 0.1808 - val_acc: 0.9315\n",
      "Epoch 620/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.0967 - acc: 0.9599 - val_loss: 0.1913 - val_acc: 0.9272\n",
      "Epoch 621/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0933 - acc: 0.9617 - val_loss: 0.1862 - val_acc: 0.9277\n",
      "Epoch 622/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0973 - acc: 0.9627 - val_loss: 0.1928 - val_acc: 0.9253\n",
      "Epoch 623/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0976 - acc: 0.9605 - val_loss: 0.1896 - val_acc: 0.9258\n",
      "Epoch 624/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0970 - acc: 0.9621 - val_loss: 0.1912 - val_acc: 0.9263\n",
      "Epoch 625/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0952 - acc: 0.9619 - val_loss: 0.1928 - val_acc: 0.9272\n",
      "Epoch 626/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.0982 - acc: 0.9609 - val_loss: 0.2002 - val_acc: 0.9253\n",
      "Epoch 627/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0946 - acc: 0.9634 - val_loss: 0.1942 - val_acc: 0.9225\n",
      "Epoch 628/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0927 - acc: 0.9619 - val_loss: 0.1833 - val_acc: 0.9301\n",
      "Epoch 629/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.0970 - acc: 0.9620 - val_loss: 0.1758 - val_acc: 0.9334\n",
      "Epoch 630/750\n",
      "13253/13253 [==============================] - 1s 56us/step - loss: 0.0986 - acc: 0.9611 - val_loss: 0.1915 - val_acc: 0.9263\n",
      "Epoch 631/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.0957 - acc: 0.9621 - val_loss: 0.1921 - val_acc: 0.9272\n",
      "Epoch 632/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1006 - acc: 0.9616 - val_loss: 0.1896 - val_acc: 0.9315\n",
      "Epoch 633/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0939 - acc: 0.9626 - val_loss: 0.1870 - val_acc: 0.9296\n",
      "Epoch 634/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0978 - acc: 0.9597 - val_loss: 0.1757 - val_acc: 0.9310\n",
      "Epoch 635/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.0948 - acc: 0.9608 - val_loss: 0.2083 - val_acc: 0.9234\n",
      "Epoch 636/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.0966 - acc: 0.9610 - val_loss: 0.1955 - val_acc: 0.9291\n",
      "Epoch 637/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0962 - acc: 0.9611 - val_loss: 0.1685 - val_acc: 0.9324\n",
      "Epoch 638/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0921 - acc: 0.9626 - val_loss: 0.1940 - val_acc: 0.9286\n",
      "Epoch 639/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0935 - acc: 0.9624 - val_loss: 0.2035 - val_acc: 0.9239\n",
      "Epoch 640/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.0963 - acc: 0.9610 - val_loss: 0.1781 - val_acc: 0.9305\n",
      "Epoch 641/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0962 - acc: 0.9599 - val_loss: 0.1964 - val_acc: 0.9220\n",
      "Epoch 642/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0944 - acc: 0.9611 - val_loss: 0.1972 - val_acc: 0.9272\n",
      "Epoch 643/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0977 - acc: 0.9602 - val_loss: 0.1882 - val_acc: 0.9225\n",
      "Epoch 644/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0974 - acc: 0.9635 - val_loss: 0.1702 - val_acc: 0.9339\n",
      "Epoch 645/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0906 - acc: 0.9625 - val_loss: 0.1845 - val_acc: 0.9329\n",
      "Epoch 646/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0899 - acc: 0.9653 - val_loss: 0.1809 - val_acc: 0.9310\n",
      "Epoch 647/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0960 - acc: 0.9620 - val_loss: 0.1826 - val_acc: 0.9296\n",
      "Epoch 648/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1049 - acc: 0.9588 - val_loss: 0.1788 - val_acc: 0.9343\n",
      "Epoch 649/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0985 - acc: 0.9622 - val_loss: 0.1983 - val_acc: 0.9225\n",
      "Epoch 650/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0961 - acc: 0.9626 - val_loss: 0.1864 - val_acc: 0.9329\n",
      "Epoch 651/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0953 - acc: 0.9628 - val_loss: 0.1853 - val_acc: 0.9348\n",
      "Epoch 652/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0970 - acc: 0.9622 - val_loss: 0.2024 - val_acc: 0.9253\n",
      "Epoch 653/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0949 - acc: 0.9628 - val_loss: 0.1944 - val_acc: 0.9282\n",
      "Epoch 654/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0928 - acc: 0.9636 - val_loss: 0.2043 - val_acc: 0.9253\n",
      "Epoch 655/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0988 - acc: 0.9620 - val_loss: 0.1916 - val_acc: 0.9282\n",
      "Epoch 656/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0939 - acc: 0.9628 - val_loss: 0.2059 - val_acc: 0.9291\n",
      "Epoch 657/750\n",
      "13253/13253 [==============================] - 1s 44us/step - loss: 0.0942 - acc: 0.9628 - val_loss: 0.1816 - val_acc: 0.9315\n",
      "Epoch 658/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.1023 - acc: 0.9606 - val_loss: 0.1770 - val_acc: 0.9348\n",
      "Epoch 659/750\n",
      "13253/13253 [==============================] - 1s 47us/step - loss: 0.0906 - acc: 0.9639 - val_loss: 0.1956 - val_acc: 0.9272\n",
      "Epoch 660/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.0936 - acc: 0.9624 - val_loss: 0.1929 - val_acc: 0.9286\n",
      "Epoch 661/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0975 - acc: 0.9635 - val_loss: 0.1927 - val_acc: 0.9244\n",
      "Epoch 662/750\n",
      "13253/13253 [==============================] - 1s 57us/step - loss: 0.0931 - acc: 0.9631 - val_loss: 0.1823 - val_acc: 0.9282\n",
      "Epoch 663/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0894 - acc: 0.9636 - val_loss: 0.1948 - val_acc: 0.9258\n",
      "Epoch 664/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0972 - acc: 0.9622 - val_loss: 0.1744 - val_acc: 0.9320\n",
      "Epoch 665/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0912 - acc: 0.9619 - val_loss: 0.1946 - val_acc: 0.9220\n",
      "Epoch 666/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0901 - acc: 0.9638 - val_loss: 0.2006 - val_acc: 0.9234\n",
      "Epoch 667/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.0935 - acc: 0.9633 - val_loss: 0.2048 - val_acc: 0.9234\n",
      "Epoch 668/750\n",
      "13253/13253 [==============================] - 1s 62us/step - loss: 0.0927 - acc: 0.9620 - val_loss: 0.1813 - val_acc: 0.9277\n",
      "Epoch 669/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0936 - acc: 0.9617 - val_loss: 0.1950 - val_acc: 0.9258\n",
      "Epoch 670/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0946 - acc: 0.9626 - val_loss: 0.2120 - val_acc: 0.9229\n",
      "Epoch 671/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0985 - acc: 0.9618 - val_loss: 0.1834 - val_acc: 0.9310\n",
      "Epoch 672/750\n",
      "13253/13253 [==============================] - 1s 48us/step - loss: 0.0916 - acc: 0.9641 - val_loss: 0.1897 - val_acc: 0.9305\n",
      "Epoch 673/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.0929 - acc: 0.9630 - val_loss: 0.1841 - val_acc: 0.9329\n",
      "Epoch 674/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0897 - acc: 0.9647 - val_loss: 0.1865 - val_acc: 0.9286\n",
      "Epoch 675/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.0944 - acc: 0.9622 - val_loss: 0.1933 - val_acc: 0.9253\n",
      "Epoch 676/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0981 - acc: 0.9626 - val_loss: 0.1916 - val_acc: 0.9244\n",
      "Epoch 677/750\n",
      "13253/13253 [==============================] - 1s 51us/step - loss: 0.0955 - acc: 0.9639 - val_loss: 0.1980 - val_acc: 0.9239\n",
      "Epoch 678/750\n",
      "13253/13253 [==============================] - 1s 55us/step - loss: 0.0968 - acc: 0.9632 - val_loss: 0.1826 - val_acc: 0.9282\n",
      "Epoch 679/750\n",
      "13253/13253 [==============================] - 1s 50us/step - loss: 0.0930 - acc: 0.9621 - val_loss: 0.1761 - val_acc: 0.9348\n",
      "Epoch 680/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0941 - acc: 0.9615 - val_loss: 0.1944 - val_acc: 0.9282\n",
      "Epoch 681/750\n",
      "13253/13253 [==============================] - 1s 53us/step - loss: 0.0943 - acc: 0.9616 - val_loss: 0.1561 - val_acc: 0.9382\n",
      "Epoch 682/750\n",
      "13253/13253 [==============================] - 1s 52us/step - loss: 0.0910 - acc: 0.9642 - val_loss: 0.1712 - val_acc: 0.9329\n",
      "Epoch 683/750\n",
      "13253/13253 [==============================] - 1s 54us/step - loss: 0.0942 - acc: 0.9636 - val_loss: 0.1694 - val_acc: 0.9296\n",
      "Epoch 684/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0959 - acc: 0.9618 - val_loss: 0.1684 - val_acc: 0.9315\n",
      "Epoch 685/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0869 - acc: 0.9657 - val_loss: 0.1974 - val_acc: 0.9272\n",
      "Epoch 686/750\n",
      "13253/13253 [==============================] - 1s 41us/step - loss: 0.0979 - acc: 0.9608 - val_loss: 0.1820 - val_acc: 0.9291\n",
      "Epoch 687/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.1056 - acc: 0.9602 - val_loss: 0.1836 - val_acc: 0.9220\n",
      "Epoch 688/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0954 - acc: 0.9615 - val_loss: 0.1603 - val_acc: 0.9339\n",
      "Epoch 689/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0915 - acc: 0.9620 - val_loss: 0.1743 - val_acc: 0.9315\n",
      "Epoch 690/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0951 - acc: 0.9608 - val_loss: 0.1928 - val_acc: 0.9277\n",
      "Epoch 691/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0941 - acc: 0.9631 - val_loss: 0.1844 - val_acc: 0.9320\n",
      "Epoch 692/750\n",
      "13253/13253 [==============================] - 1s 49us/step - loss: 0.0964 - acc: 0.9635 - val_loss: 0.1750 - val_acc: 0.9363\n",
      "Epoch 693/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0921 - acc: 0.9638 - val_loss: 0.2062 - val_acc: 0.9267\n",
      "Epoch 694/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0947 - acc: 0.9628 - val_loss: 0.1976 - val_acc: 0.9291\n",
      "Epoch 695/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0906 - acc: 0.9633 - val_loss: 0.1819 - val_acc: 0.9339\n",
      "Epoch 696/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0913 - acc: 0.9634 - val_loss: 0.1657 - val_acc: 0.9310\n",
      "Epoch 697/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0952 - acc: 0.9593 - val_loss: 0.1869 - val_acc: 0.9277\n",
      "Epoch 698/750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0970 - acc: 0.9614 - val_loss: 0.1799 - val_acc: 0.9343\n",
      "Epoch 699/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0940 - acc: 0.9611 - val_loss: 0.1838 - val_acc: 0.9315\n",
      "Epoch 700/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0929 - acc: 0.9632 - val_loss: 0.1777 - val_acc: 0.9334\n",
      "Epoch 701/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0911 - acc: 0.9640 - val_loss: 0.1638 - val_acc: 0.9339\n",
      "Epoch 702/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0910 - acc: 0.9647 - val_loss: 0.1790 - val_acc: 0.9291\n",
      "Epoch 703/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0917 - acc: 0.9630 - val_loss: 0.1904 - val_acc: 0.9263\n",
      "Epoch 704/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0972 - acc: 0.9625 - val_loss: 0.1715 - val_acc: 0.9286\n",
      "Epoch 705/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0914 - acc: 0.9637 - val_loss: 0.1864 - val_acc: 0.9258\n",
      "Epoch 706/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0879 - acc: 0.9656 - val_loss: 0.1953 - val_acc: 0.9253\n",
      "Epoch 707/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0951 - acc: 0.9599 - val_loss: 0.1877 - val_acc: 0.9305\n",
      "Epoch 708/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0959 - acc: 0.9612 - val_loss: 0.1611 - val_acc: 0.9363\n",
      "Epoch 709/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0962 - acc: 0.9604 - val_loss: 0.1675 - val_acc: 0.9348\n",
      "Epoch 710/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0912 - acc: 0.9633 - val_loss: 0.1737 - val_acc: 0.9305\n",
      "Epoch 711/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0901 - acc: 0.9624 - val_loss: 0.1792 - val_acc: 0.9291\n",
      "Epoch 712/750\n",
      "13253/13253 [==============================] - 0s 34us/step - loss: 0.0926 - acc: 0.9628 - val_loss: 0.1676 - val_acc: 0.9320\n",
      "Epoch 713/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0936 - acc: 0.9646 - val_loss: 0.1731 - val_acc: 0.9272\n",
      "Epoch 714/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0942 - acc: 0.9638 - val_loss: 0.1684 - val_acc: 0.9324\n",
      "Epoch 715/750\n",
      "13253/13253 [==============================] - 1s 46us/step - loss: 0.0963 - acc: 0.9612 - val_loss: 0.1614 - val_acc: 0.9305\n",
      "Epoch 716/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0980 - acc: 0.9617 - val_loss: 0.1713 - val_acc: 0.9291\n",
      "Epoch 717/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0951 - acc: 0.9615 - val_loss: 0.1863 - val_acc: 0.9258\n",
      "Epoch 718/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0900 - acc: 0.9638 - val_loss: 0.1948 - val_acc: 0.9282\n",
      "Epoch 719/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0923 - acc: 0.9633 - val_loss: 0.1938 - val_acc: 0.9267\n",
      "Epoch 720/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0927 - acc: 0.9652 - val_loss: 0.1763 - val_acc: 0.9320\n",
      "Epoch 721/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0924 - acc: 0.9624 - val_loss: 0.1825 - val_acc: 0.9305\n",
      "Epoch 722/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0938 - acc: 0.9633 - val_loss: 0.1880 - val_acc: 0.9239\n",
      "Epoch 723/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0928 - acc: 0.9625 - val_loss: 0.1706 - val_acc: 0.9315\n",
      "Epoch 724/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0908 - acc: 0.9633 - val_loss: 0.1757 - val_acc: 0.9282\n",
      "Epoch 725/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0926 - acc: 0.9633 - val_loss: 0.1784 - val_acc: 0.9282\n",
      "Epoch 726/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0915 - acc: 0.9641 - val_loss: 0.1869 - val_acc: 0.9324\n",
      "Epoch 727/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0935 - acc: 0.9643 - val_loss: 0.1885 - val_acc: 0.9310\n",
      "Epoch 728/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0897 - acc: 0.9641 - val_loss: 0.1753 - val_acc: 0.9301\n",
      "Epoch 729/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0936 - acc: 0.9623 - val_loss: 0.1843 - val_acc: 0.9296\n",
      "Epoch 730/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0939 - acc: 0.9623 - val_loss: 0.1835 - val_acc: 0.9286\n",
      "Epoch 731/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0935 - acc: 0.9636 - val_loss: 0.1749 - val_acc: 0.9324\n",
      "Epoch 732/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0885 - acc: 0.9663 - val_loss: 0.1847 - val_acc: 0.9291\n",
      "Epoch 733/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0926 - acc: 0.9654 - val_loss: 0.1844 - val_acc: 0.9296\n",
      "Epoch 734/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0920 - acc: 0.9636 - val_loss: 0.1805 - val_acc: 0.9253\n",
      "Epoch 735/750\n",
      "13253/13253 [==============================] - 1s 45us/step - loss: 0.0907 - acc: 0.9636 - val_loss: 0.1836 - val_acc: 0.9267\n",
      "Epoch 736/750\n",
      "13253/13253 [==============================] - 1s 40us/step - loss: 0.0951 - acc: 0.9631 - val_loss: 0.1599 - val_acc: 0.9320\n",
      "Epoch 737/750\n",
      "13253/13253 [==============================] - 1s 39us/step - loss: 0.0946 - acc: 0.9614 - val_loss: 0.1794 - val_acc: 0.9329\n",
      "Epoch 738/750\n",
      "13253/13253 [==============================] - 1s 43us/step - loss: 0.0948 - acc: 0.9617 - val_loss: 0.1848 - val_acc: 0.9286\n",
      "Epoch 739/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0918 - acc: 0.9633 - val_loss: 0.1650 - val_acc: 0.9353\n",
      "Epoch 740/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0909 - acc: 0.9641 - val_loss: 0.1940 - val_acc: 0.9258\n",
      "Epoch 741/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0950 - acc: 0.9627 - val_loss: 0.1668 - val_acc: 0.9372\n",
      "Epoch 742/750\n",
      "13253/13253 [==============================] - 1s 42us/step - loss: 0.0942 - acc: 0.9622 - val_loss: 0.1796 - val_acc: 0.9277\n",
      "Epoch 743/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0950 - acc: 0.9619 - val_loss: 0.1674 - val_acc: 0.9363\n",
      "Epoch 744/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0914 - acc: 0.9626 - val_loss: 0.1850 - val_acc: 0.9324\n",
      "Epoch 745/750\n",
      "13253/13253 [==============================] - 0s 36us/step - loss: 0.0914 - acc: 0.9636 - val_loss: 0.1751 - val_acc: 0.9282\n",
      "Epoch 746/750\n",
      "13253/13253 [==============================] - 0s 35us/step - loss: 0.0925 - acc: 0.9630 - val_loss: 0.1691 - val_acc: 0.9339\n",
      "Epoch 747/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0907 - acc: 0.9629 - val_loss: 0.1747 - val_acc: 0.9301\n",
      "Epoch 748/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0935 - acc: 0.9631 - val_loss: 0.2007 - val_acc: 0.9253\n",
      "Epoch 749/750\n",
      "13253/13253 [==============================] - 1s 38us/step - loss: 0.0900 - acc: 0.9624 - val_loss: 0.1844 - val_acc: 0.9277\n",
      "Epoch 750/750\n",
      "13253/13253 [==============================] - 0s 37us/step - loss: 0.0925 - acc: 0.9619 - val_loss: 0.1791 - val_acc: 0.9348\n",
      "CPU times: user 11min 4s, sys: 1min 37s, total: 12min 42s\n",
      "Wall time: 6min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hist = model.fit(X_traino, y_traino,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=750,\n",
    "                 batch_size=64,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1274b8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1274b8f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9348239774481667"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss += hist.history['loss']\n",
    "test_loss += hist.history['val_loss']\n",
    "train_acc += hist.history['acc']\n",
    "test_acc += hist.history['val_acc']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best') ## I love this loc = 'best' command.\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()\n",
    "test_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9818683826213946"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.02008032e-04,\n",
       "        5.02008032e-04, 1.00401606e-03, 1.00401606e-03, 1.50602410e-03,\n",
       "        1.50602410e-03, 3.01204819e-03, 3.01204819e-03, 3.51405622e-03,\n",
       "        3.51405622e-03, 4.01606426e-03, 4.01606426e-03, 5.52208835e-03,\n",
       "        5.52208835e-03, 6.02409639e-03, 6.02409639e-03, 6.52610442e-03,\n",
       "        6.52610442e-03, 7.02811245e-03, 7.02811245e-03, 7.53012048e-03,\n",
       "        7.53012048e-03, 8.03212851e-03, 8.03212851e-03, 9.53815261e-03,\n",
       "        9.53815261e-03, 1.20481928e-02, 1.20481928e-02, 1.35542169e-02,\n",
       "        1.35542169e-02, 1.55622490e-02, 1.55622490e-02, 1.75702811e-02,\n",
       "        1.75702811e-02, 1.90763052e-02, 1.90763052e-02, 2.15863454e-02,\n",
       "        2.15863454e-02, 2.25903614e-02, 2.25903614e-02, 2.40963855e-02,\n",
       "        2.40963855e-02, 2.56024096e-02, 2.56024096e-02, 2.61044177e-02,\n",
       "        2.61044177e-02, 2.66064257e-02, 2.66064257e-02, 2.86144578e-02,\n",
       "        2.86144578e-02, 2.96184739e-02, 2.96184739e-02, 3.01204819e-02,\n",
       "        3.01204819e-02, 3.31325301e-02, 3.31325301e-02, 3.41365462e-02,\n",
       "        3.41365462e-02, 3.61445783e-02, 3.61445783e-02, 3.86546185e-02,\n",
       "        3.86546185e-02, 3.91566265e-02, 3.91566265e-02, 4.61847390e-02,\n",
       "        4.61847390e-02, 5.12048193e-02, 5.12048193e-02, 5.32128514e-02,\n",
       "        5.32128514e-02, 5.37148594e-02, 5.37148594e-02, 5.82329317e-02,\n",
       "        5.82329317e-02, 6.07429719e-02, 6.07429719e-02, 6.27510040e-02,\n",
       "        6.27510040e-02, 6.97791165e-02, 6.97791165e-02, 7.02811245e-02,\n",
       "        7.02811245e-02, 7.47991968e-02, 7.47991968e-02, 7.58032129e-02,\n",
       "        7.58032129e-02, 7.93172691e-02, 7.93172691e-02, 8.88554217e-02,\n",
       "        9.23694779e-02, 9.28714859e-02, 9.28714859e-02, 8.39859438e-01,\n",
       "        1.00000000e+00]),\n",
       " array([0.00909091, 0.06363636, 0.09090909, 0.09090909, 0.12727273,\n",
       "        0.12727273, 0.18181818, 0.18181818, 0.2       , 0.2       ,\n",
       "        0.21818182, 0.21818182, 0.35454545, 0.35454545, 0.36363636,\n",
       "        0.36363636, 0.37272727, 0.37272727, 0.38181818, 0.38181818,\n",
       "        0.39090909, 0.39090909, 0.41818182, 0.41818182, 0.48181818,\n",
       "        0.48181818, 0.51818182, 0.51818182, 0.56363636, 0.56363636,\n",
       "        0.61818182, 0.61818182, 0.64545455, 0.64545455, 0.65454545,\n",
       "        0.65454545, 0.67272727, 0.67272727, 0.68181818, 0.68181818,\n",
       "        0.7       , 0.7       , 0.70909091, 0.70909091, 0.71818182,\n",
       "        0.71818182, 0.74545455, 0.74545455, 0.75454545, 0.75454545,\n",
       "        0.76363636, 0.76363636, 0.78181818, 0.78181818, 0.79090909,\n",
       "        0.79090909, 0.8       , 0.8       , 0.80909091, 0.80909091,\n",
       "        0.82727273, 0.82727273, 0.83636364, 0.83636364, 0.84545455,\n",
       "        0.84545455, 0.86363636, 0.86363636, 0.87272727, 0.87272727,\n",
       "        0.89090909, 0.89090909, 0.9       , 0.9       , 0.90909091,\n",
       "        0.90909091, 0.91818182, 0.91818182, 0.92727273, 0.92727273,\n",
       "        0.94545455, 0.94545455, 0.95454545, 0.95454545, 0.96363636,\n",
       "        0.96363636, 0.97272727, 0.97272727, 0.98181818, 0.98181818,\n",
       "        0.99090909, 0.99090909, 0.99090909, 0.99090909, 1.        ,\n",
       "        1.        , 1.        ]),\n",
       " array([1.0000000e+00, 9.9999917e-01, 9.9998963e-01, 9.9998689e-01,\n",
       "        9.9992418e-01, 9.9992144e-01, 9.9987769e-01, 9.9986744e-01,\n",
       "        9.9985003e-01, 9.9975652e-01, 9.9970502e-01, 9.9968731e-01,\n",
       "        9.9896073e-01, 9.9890482e-01, 9.9882597e-01, 9.9838185e-01,\n",
       "        9.9835509e-01, 9.9821717e-01, 9.9820125e-01, 9.9795073e-01,\n",
       "        9.9794525e-01, 9.9784636e-01, 9.9754912e-01, 9.9732888e-01,\n",
       "        9.9598569e-01, 9.9591702e-01, 9.9468935e-01, 9.9305528e-01,\n",
       "        9.9048901e-01, 9.8633236e-01, 9.8177779e-01, 9.7351587e-01,\n",
       "        9.6534246e-01, 9.6282744e-01, 9.6154588e-01, 9.5496649e-01,\n",
       "        9.5394599e-01, 9.4199252e-01, 9.4069707e-01, 9.2983741e-01,\n",
       "        9.2878103e-01, 9.2323285e-01, 9.2311448e-01, 9.1905266e-01,\n",
       "        9.0880460e-01, 8.9114898e-01, 8.9044189e-01, 8.8052124e-01,\n",
       "        8.7680984e-01, 8.7108934e-01, 8.7106442e-01, 8.6050773e-01,\n",
       "        8.5721672e-01, 8.4455562e-01, 8.4065789e-01, 8.3372372e-01,\n",
       "        8.3366281e-01, 8.2285398e-01, 8.2285094e-01, 8.2100374e-01,\n",
       "        8.1878507e-01, 7.9752672e-01, 7.8608161e-01, 7.6566535e-01,\n",
       "        7.6237679e-01, 7.5802702e-01, 7.5227761e-01, 6.5786469e-01,\n",
       "        6.5780950e-01, 6.2986529e-01, 6.2163049e-01, 5.9439927e-01,\n",
       "        5.8231699e-01, 5.6709850e-01, 5.6702435e-01, 5.5993330e-01,\n",
       "        5.3906655e-01, 4.9540958e-01, 4.9018160e-01, 4.7020519e-01,\n",
       "        4.6791303e-01, 4.5129430e-01, 4.5128343e-01, 4.5126164e-01,\n",
       "        4.5122910e-01, 4.0196463e-01, 3.9872661e-01, 3.9610702e-01,\n",
       "        3.9610067e-01, 3.9590347e-01, 3.9582714e-01, 2.4951094e-01,\n",
       "        2.4771337e-01, 2.4119334e-01, 2.3783574e-01, 1.7542442e-38,\n",
       "        0.0000000e+00], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_curve(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.0\n",
      "Confusion Matrix\n",
      "[[ 197 1795]\n",
      " [   0  110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.10      0.18      1992\n",
      "        Yes       0.06      1.00      0.11       110\n",
      "\n",
      "avg / total       0.95      0.15      0.18      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.01\n",
      "Confusion Matrix\n",
      "[[1626  366]\n",
      " [   0  110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.82      0.90      1992\n",
      "        Yes       0.23      1.00      0.38       110\n",
      "\n",
      "avg / total       0.96      0.83      0.87      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.02\n",
      "Confusion Matrix\n",
      "[[1673  319]\n",
      " [   0  110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.84      0.91      1992\n",
      "        Yes       0.26      1.00      0.41       110\n",
      "\n",
      "avg / total       0.96      0.85      0.89      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.03\n",
      "Confusion Matrix\n",
      "[[1691  301]\n",
      " [   0  110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.85      0.92      1992\n",
      "        Yes       0.27      1.00      0.42       110\n",
      "\n",
      "avg / total       0.96      0.86      0.89      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.04\n",
      "Confusion Matrix\n",
      "[[1713  279]\n",
      " [   0  110]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.86      0.92      1992\n",
      "        Yes       0.28      1.00      0.44       110\n",
      "\n",
      "avg / total       0.96      0.87      0.90      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.05\n",
      "Confusion Matrix\n",
      "[[1723  269]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.86      0.93      1992\n",
      "        Yes       0.29      0.99      0.45       110\n",
      "\n",
      "avg / total       0.96      0.87      0.90      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.06\n",
      "Confusion Matrix\n",
      "[[1731  261]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.87      0.93      1992\n",
      "        Yes       0.29      0.99      0.45       110\n",
      "\n",
      "avg / total       0.96      0.88      0.90      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.07\n",
      "Confusion Matrix\n",
      "[[1737  255]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.87      0.93      1992\n",
      "        Yes       0.30      0.99      0.46       110\n",
      "\n",
      "avg / total       0.96      0.88      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.08\n",
      "Confusion Matrix\n",
      "[[1743  249]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.88      0.93      1992\n",
      "        Yes       0.30      0.99      0.47       110\n",
      "\n",
      "avg / total       0.96      0.88      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.09\n",
      "Confusion Matrix\n",
      "[[1749  243]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.88      0.93      1992\n",
      "        Yes       0.31      0.99      0.47       110\n",
      "\n",
      "avg / total       0.96      0.88      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.1\n",
      "Confusion Matrix\n",
      "[[1754  238]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.88      0.94      1992\n",
      "        Yes       0.31      0.99      0.48       110\n",
      "\n",
      "avg / total       0.96      0.89      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.11\n",
      "Confusion Matrix\n",
      "[[1758  234]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.88      0.94      1992\n",
      "        Yes       0.32      0.99      0.48       110\n",
      "\n",
      "avg / total       0.96      0.89      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.12\n",
      "Confusion Matrix\n",
      "[[1760  232]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.88      0.94      1992\n",
      "        Yes       0.32      0.99      0.48       110\n",
      "\n",
      "avg / total       0.96      0.89      0.91      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.13\n",
      "Confusion Matrix\n",
      "[[1764  228]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.32      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.14\n",
      "Confusion Matrix\n",
      "[[1765  227]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.32      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.15\n",
      "Confusion Matrix\n",
      "[[1765  227]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.32      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.16\n",
      "Confusion Matrix\n",
      "[[1766  226]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.33      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.17\n",
      "Confusion Matrix\n",
      "[[1766  226]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.33      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.18\n",
      "Confusion Matrix\n",
      "[[1769  223]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.33      0.99      0.49       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.19\n",
      "Confusion Matrix\n",
      "[[1772  220]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.33      0.99      0.50       110\n",
      "\n",
      "avg / total       0.96      0.89      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.2\n",
      "Confusion Matrix\n",
      "[[1778  214]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.34      0.99      0.50       110\n",
      "\n",
      "avg / total       0.96      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.21\n",
      "Confusion Matrix\n",
      "[[1778  214]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.34      0.99      0.50       110\n",
      "\n",
      "avg / total       0.96      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.22\n",
      "Confusion Matrix\n",
      "[[1780  212]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.34      0.99      0.51       110\n",
      "\n",
      "avg / total       0.96      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.23\n",
      "Confusion Matrix\n",
      "[[1780  212]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.89      0.94      1992\n",
      "        Yes       0.34      0.99      0.51       110\n",
      "\n",
      "avg / total       0.96      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.24\n",
      "Confusion Matrix\n",
      "[[1783  209]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.94      1992\n",
      "        Yes       0.34      0.99      0.51       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.25\n",
      "Confusion Matrix\n",
      "[[1784  208]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.94      1992\n",
      "        Yes       0.34      0.99      0.51       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.26\n",
      "Confusion Matrix\n",
      "[[1785  207]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.94      1992\n",
      "        Yes       0.34      0.99      0.51       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.27\n",
      "Confusion Matrix\n",
      "[[1788  204]\n",
      " [   1  109]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.35      0.99      0.52       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.28\n",
      "Confusion Matrix\n",
      "[[1791  201]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.35      0.99      0.52       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.29\n",
      "Confusion Matrix\n",
      "[[1791  201]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.35      0.99      0.52       110\n",
      "\n",
      "avg / total       0.97      0.90      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.3\n",
      "Confusion Matrix\n",
      "[[1794  198]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.36      0.99      0.52       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.31\n",
      "Confusion Matrix\n",
      "[[1799  193]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.36      0.99      0.53       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.32\n",
      "Confusion Matrix\n",
      "[[1801  191]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.90      0.95      1992\n",
      "        Yes       0.36      0.99      0.53       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.33\n",
      "Confusion Matrix\n",
      "[[1804  188]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.91      0.95      1992\n",
      "        Yes       0.37      0.99      0.54       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.34\n",
      "Confusion Matrix\n",
      "[[1809  183]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.91      0.95      1992\n",
      "        Yes       0.37      0.99      0.54       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.35000000000000003\n",
      "Confusion Matrix\n",
      "[[1811  181]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.91      0.95      1992\n",
      "        Yes       0.38      0.99      0.54       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.36\n",
      "Confusion Matrix\n",
      "[[1811  181]\n",
      " [   1  109]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.91      0.95      1992\n",
      "        Yes       0.38      0.99      0.54       110\n",
      "\n",
      "avg / total       0.97      0.91      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.37\n",
      "Confusion Matrix\n",
      "[[1816  176]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.91      0.95      1992\n",
      "        Yes       0.38      0.98      0.55       110\n",
      "\n",
      "avg / total       0.97      0.92      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.38\n",
      "Confusion Matrix\n",
      "[[1823  169]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.39      0.98      0.56       110\n",
      "\n",
      "avg / total       0.97      0.92      0.93      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.39\n",
      "Confusion Matrix\n",
      "[[1825  167]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.39      0.98      0.56       110\n",
      "\n",
      "avg / total       0.97      0.92      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.4\n",
      "Confusion Matrix\n",
      "[[1826  166]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.39      0.98      0.56       110\n",
      "\n",
      "avg / total       0.97      0.92      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.41000000000000003\n",
      "Confusion Matrix\n",
      "[[1827  165]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.40      0.98      0.56       110\n",
      "\n",
      "avg / total       0.97      0.92      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.42\n",
      "Confusion Matrix\n",
      "[[1833  159]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.40      0.98      0.57       110\n",
      "\n",
      "avg / total       0.97      0.92      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.43\n",
      "Confusion Matrix\n",
      "[[1837  155]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.41      0.98      0.58       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.44\n",
      "Confusion Matrix\n",
      "[[1838  154]\n",
      " [   2  108]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.92      0.96      1992\n",
      "        Yes       0.41      0.98      0.58       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.45\n",
      "Confusion Matrix\n",
      "[[1846  146]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.42      0.96      0.59       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.46\n",
      "Confusion Matrix\n",
      "[[1848  144]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.42      0.96      0.59       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.47000000000000003\n",
      "Confusion Matrix\n",
      "[[1850  142]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.43      0.96      0.59       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.48\n",
      "Confusion Matrix\n",
      "[[1854  138]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.43      0.96      0.60       110\n",
      "\n",
      "avg / total       0.97      0.93      0.94      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.49\n",
      "Confusion Matrix\n",
      "[[1858  134]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.44      0.96      0.61       110\n",
      "\n",
      "avg / total       0.97      0.93      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.5\n",
      "Confusion Matrix\n",
      "[[1859  133]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.44      0.96      0.61       110\n",
      "\n",
      "avg / total       0.97      0.93      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.51\n",
      "Confusion Matrix\n",
      "[[1861  131]\n",
      " [   4  106]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.45      0.96      0.61       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.52\n",
      "Confusion Matrix\n",
      "[[1861  131]\n",
      " [   5  105]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.93      0.96      1992\n",
      "        Yes       0.44      0.95      0.61       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.53\n",
      "Confusion Matrix\n",
      "[[1871  121]\n",
      " [   7  103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.46      0.94      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.54\n",
      "Confusion Matrix\n",
      "[[1871  121]\n",
      " [   7  103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.46      0.94      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.55\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1872  120]\n",
      " [   7  103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.46      0.94      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.56\n",
      "Confusion Matrix\n",
      "[[1876  116]\n",
      " [   7  103]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.47      0.94      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.5700000000000001\n",
      "Confusion Matrix\n",
      "[[1876  116]\n",
      " [   8  102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.47      0.93      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.58\n",
      "Confusion Matrix\n",
      "[[1879  113]\n",
      " [   8  102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.47      0.93      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.59\n",
      "Confusion Matrix\n",
      "[[1882  110]\n",
      " [   8  102]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       1.00      0.94      0.97      1992\n",
      "        Yes       0.48      0.93      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.6\n",
      "Confusion Matrix\n",
      "[[1884  108]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.61\n",
      "Confusion Matrix\n",
      "[[1884  108]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.62\n",
      "Confusion Matrix\n",
      "[[1884  108]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.62       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.63\n",
      "Confusion Matrix\n",
      "[[1885  107]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.64\n",
      "Confusion Matrix\n",
      "[[1886  106]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.65\n",
      "Confusion Matrix\n",
      "[[1886  106]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.48      0.90      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.66\n",
      "Confusion Matrix\n",
      "[[1888  104]\n",
      " [  11   99]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.49      0.90      0.63       110\n",
      "\n",
      "avg / total       0.97      0.95      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.67\n",
      "Confusion Matrix\n",
      "[[1888  104]\n",
      " [  12   98]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.49      0.89      0.63       110\n",
      "\n",
      "avg / total       0.97      0.94      0.95      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.68\n",
      "Confusion Matrix\n",
      "[[1901   91]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.95      0.97      1992\n",
      "        Yes       0.51      0.87      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.6900000000000001\n",
      "Confusion Matrix\n",
      "[[1903   89]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.52      0.87      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.7000000000000001\n",
      "Confusion Matrix\n",
      "[[1903   89]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.52      0.87      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.71\n",
      "Confusion Matrix\n",
      "[[1905   87]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.52      0.87      0.66       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.72\n",
      "Confusion Matrix\n",
      "[[1906   86]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.53      0.87      0.66       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.73\n",
      "Confusion Matrix\n",
      "[[1906   86]\n",
      " [  14   96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.53      0.87      0.66       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.74\n",
      "Confusion Matrix\n",
      "[[1907   85]\n",
      " [  16   94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.97      1992\n",
      "        Yes       0.53      0.85      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.75\n",
      "Confusion Matrix\n",
      "[[1911   81]\n",
      " [  17   93]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.53      0.85      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.76\n",
      "Confusion Matrix\n",
      "[[1914   78]\n",
      " [  18   92]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.54      0.84      0.66       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.77\n",
      "Confusion Matrix\n",
      "[[1915   77]\n",
      " [  19   91]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.54      0.83      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.78\n",
      "Confusion Matrix\n",
      "[[1918   74]\n",
      " [  21   89]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.55      0.81      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.79\n",
      "Confusion Matrix\n",
      "[[1919   73]\n",
      " [  21   89]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.55      0.81      0.65       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.8\n",
      "Confusion Matrix\n",
      "[[1920   72]\n",
      " [  21   89]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.55      0.81      0.66       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.81\n",
      "Confusion Matrix\n",
      "[[1920   72]\n",
      " [  23   87]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.96      0.98      1992\n",
      "        Yes       0.55      0.79      0.65       110\n",
      "\n",
      "avg / total       0.97      0.95      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.8200000000000001\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1923   69]\n",
      " [  23   87]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.56      0.79      0.65       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.8300000000000001\n",
      "Confusion Matrix\n",
      "[[1926   66]\n",
      " [  23   87]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.57      0.79      0.66       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.84\n",
      "Confusion Matrix\n",
      "[[1932   60]\n",
      " [  26   84]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.58      0.76      0.66       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.85\n",
      "Confusion Matrix\n",
      "[[1935   57]\n",
      " [  26   84]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.60      0.76      0.67       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.86\n",
      "Confusion Matrix\n",
      "[[1938   54]\n",
      " [  28   82]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.60      0.75      0.67       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.87\n",
      "Confusion Matrix\n",
      "[[1938   54]\n",
      " [  28   82]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.99      0.97      0.98      1992\n",
      "        Yes       0.60      0.75      0.67       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.88\n",
      "Confusion Matrix\n",
      "[[1941   51]\n",
      " [  30   80]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.97      0.98      1992\n",
      "        Yes       0.61      0.73      0.66       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.89\n",
      "Confusion Matrix\n",
      "[[1943   49]\n",
      " [  30   80]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.62      0.73      0.67       110\n",
      "\n",
      "avg / total       0.97      0.96      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.9\n",
      "Confusion Matrix\n",
      "[[1947   45]\n",
      " [  30   80]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.64      0.73      0.68       110\n",
      "\n",
      "avg / total       0.97      0.96      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.91\n",
      "Confusion Matrix\n",
      "[[1948   44]\n",
      " [  31   79]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.64      0.72      0.68       110\n",
      "\n",
      "avg / total       0.97      0.96      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.92\n",
      "Confusion Matrix\n",
      "[[1954   38]\n",
      " [  34   76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.67      0.69      0.68       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.93\n",
      "Confusion Matrix\n",
      "[[1956   36]\n",
      " [  34   76]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.68      0.69      0.68       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.9400000000000001\n",
      "Confusion Matrix\n",
      "[[1957   35]\n",
      " [  36   74]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.68      0.67      0.68       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.9500000000000001\n",
      "Confusion Matrix\n",
      "[[1960   32]\n",
      " [  37   73]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.98      0.98      1992\n",
      "        Yes       0.70      0.66      0.68       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.96\n",
      "Confusion Matrix\n",
      "[[1964   28]\n",
      " [  38   72]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.99      0.98      1992\n",
      "        Yes       0.72      0.65      0.69       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.97\n",
      "Confusion Matrix\n",
      "[[1964   28]\n",
      " [  40   70]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.99      0.98      1992\n",
      "        Yes       0.71      0.64      0.67       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.98\n",
      "Confusion Matrix\n",
      "[[1970   22]\n",
      " [  45   65]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.98      0.99      0.98      1992\n",
      "        Yes       0.75      0.59      0.66       110\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 0.99\n",
      "Confusion Matrix\n",
      "[[1977   15]\n",
      " [  56   54]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.97      0.99      0.98      1992\n",
      "        Yes       0.78      0.49      0.60       110\n",
      "\n",
      "avg / total       0.96      0.97      0.96      2102\n",
      "\n",
      "*******************************\n",
      "\n",
      "Threshold: 1.0\n",
      "Confusion Matrix\n",
      "[[1992    0]\n",
      " [ 110    0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         No       0.95      1.00      0.97      1992\n",
      "        Yes       0.00      0.00      0.00       110\n",
      "\n",
      "avg / total       0.90      0.95      0.92      2102\n",
      "\n",
      "*******************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evankranzler/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for threshold in np.linspace(0, 1, 101):\n",
    "    print('Threshold: {}'.format(threshold))\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test, (model.predict(X_test) > threshold).astype(float)))\n",
    "    print(classification_report(y_test, (model.predict(X_test) > threshold).astype(float), target_names=['No', 'Yes']))\n",
    "    print('*******************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3992154496077248"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_testo.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126893908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1272cc2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sens = []\n",
    "spec = []\n",
    "most_goodest = []\n",
    "pred = model.predict(X_test)\n",
    "for n in np.linspace(0, 1, 10001):\n",
    "    temp = 1 * (pred > n)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, temp).ravel()\n",
    "    sens += [tp / (tp + fn)]\n",
    "    spec += [1 - (tn / (tn + fp))]\n",
    "    most_goodest += [tp + tn]\n",
    "# sens += [0]\n",
    "# spec += [0]\n",
    "plt.plot(spec, sens);\n",
    "plt.show()\n",
    "plt.plot(np.linspace(0, 1, 10001), most_goodest)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2037, 0.9792000000000001)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([x for x in zip(most_goodest, np.linspace(0, 1, 10001))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1970, 22, 43, 67)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 1 * (pred > .9792)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, temp).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.001, 0.002, ..., 0.998, 0.999, 1.   ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 1, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "5         0\n",
       "6         0\n",
       "7         0\n",
       "8         0\n",
       "9         0\n",
       "10        0\n",
       "11        0\n",
       "12        0\n",
       "13        0\n",
       "14        0\n",
       "15        0\n",
       "16        0\n",
       "17        0\n",
       "18        0\n",
       "19        0\n",
       "20        0\n",
       "21        0\n",
       "22        0\n",
       "23        0\n",
       "24        0\n",
       "25        0\n",
       "26        0\n",
       "27        0\n",
       "28        0\n",
       "29        0\n",
       "         ..\n",
       "116263    0\n",
       "116264    0\n",
       "116265    0\n",
       "116266    0\n",
       "116267    0\n",
       "116268    0\n",
       "116269    0\n",
       "116270    0\n",
       "116271    0\n",
       "116272    0\n",
       "116273    0\n",
       "116274    0\n",
       "116275    0\n",
       "116276    0\n",
       "116277    0\n",
       "116278    0\n",
       "116279    0\n",
       "116280    0\n",
       "116281    0\n",
       "116282    0\n",
       "116283    0\n",
       "116284    0\n",
       "116285    0\n",
       "116286    0\n",
       "116287    0\n",
       "116288    0\n",
       "116289    0\n",
       "116290    0\n",
       "116291    0\n",
       "116292    0\n",
       "Name: WnvPresent, Length: 116293, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.WnvPresent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "submission = pd.DataFrame([[i,x[0]] for i, x in enumerate(preds.tolist())], columns=sample.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'WnvPresent'], dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
